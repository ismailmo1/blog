[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "Does Darwin Nunez only score difficult chances?\n\n\n12 min\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\ndata-viz\n\n\nfootball\n\n\npremier league\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorld Cup 2022 Fatigue\n\n\n8 min\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\ndata-viz\n\n\nfootball\n\n\nworld cup\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsync web scraping and Streamlit\n\n\n6 min\n\n\n\npython\n\n\nstreamlit\n\n\nasyncio\n\n\nweb-scraping\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Mystery of 2019 Champions League Final\n\n\n5 min\n\n\n\nweb-scraping\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\ndata-viz\n\n\nfootball\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytest and SQL\n\n\n14 min\n\n\n\nsql\n\n\npython\n\n\npytest\n\n\n\n\nSep 11, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\nAny thoughts, comments or ideas? Please let me know here!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi I’m Ismail, thanks for checking out my blog. I’m currently working as software engineer at Amey and I started writing just so I could get better at writing but if you’ve stumbled across this somehow, it’d be great to hear from you!\nYou can reach out to me through any of the links below:\n\n\n\n\n\n\n  \n\n\n\n\nAsync web scraping and Streamlit\n\n\n\n\nSpeeding up web scraping in a Streamlit analytics app\n\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n  \n\n\n\n\nDoes Darwin Nunez only score difficult chances?\n\n\n\n\nAnalysing expected goals (xG) of missed chances and scored goals.\n\n\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Blog\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPytest and SQL\n\n\n\n\nAutomating the testing of your SQL queries\n\n\n\n\n\n\nSep 11, 2022\n\n\n\n\n\n\n  \n\n\n\n\nThe Mystery of 2019 Champions League Final\n\n\n\n\nExploring a strange statistic about the Liverpool starting 11 in the Champions League final\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWorld Cup 2022 Fatigue\n\n\n\n\nAssessing the impact of the World Cup on the Premier League\n\n\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\nNo matching items\n\nAny thoughts, comments or ideas? Please let me know here!"
  },
  {
    "objectID": "posts/worldcup-fatigue/index.html",
    "href": "posts/worldcup-fatigue/index.html",
    "title": "World Cup 2022 Fatigue",
    "section": "",
    "text": "The 2022 World Cup in Qatar was the first to be held during the winter months in the Premier League to avoid the intense heat of the Middle Eastern summer, so the Premier League season was put on hold accomodate for this. With only a week break between the end of the World Cup and the restart of the Premier League, it’s clear that some players and clubs will be impacted more than others, depending on how many games they played during the international tournament.\nThis post aims to explore the impact of the World Cup on the Premier League.\nAny thoughts, comments or ideas? Please let me know here!"
  },
  {
    "objectID": "posts/worldcup-fatigue/index.html#adding-country-name",
    "href": "posts/worldcup-fatigue/index.html#adding-country-name",
    "title": "World Cup 2022 Fatigue",
    "section": "3.1 Adding country name",
    "text": "3.1 Adding country name\nThis data doesn’t currently contain the name of the country, and although we can get this by scraping more webpages, it isn’t ideal since we have to make more requests which is slow and writing more code to scrape the responses won’t be fun or productive.\nWe do have the data for each game played by the player, so a hacky way to get the team name is to calculate the mode of the teams in the subset of data for that player. I.e. for Lucas Digne (France), here are all the games played by him:\n\nprem_world_cup_minutes_df[prem_world_cup_minutes_df['player_name'] ==\"Lucas Digne\"]\n\n\n\n\n\n  \n    \n      \n      Date\n      Matchday\n      Home team.1\n      Away team.1\n      Result\n      min_played\n      subbed_on\n      subbed_off\n      player_name\n      club\n    \n  \n  \n    \n      0\n      2022-11-22\n      Group D\n      France\n      Australia\n      4:1\n      0\n      Not in squad\n      Not in squad\n      Lucas Digne\n      Aston Villa\n    \n    \n      1\n      2022-11-26\n      Group D\n      France\n      Denmark\n      2:1\n      0\n      Not in squad\n      Not in squad\n      Lucas Digne\n      Aston Villa\n    \n    \n      2\n      2022-11-30\n      Group D\n      Tunisia\n      France\n      1:0\n      0\n      Not in squad\n      Not in squad\n      Lucas Digne\n      Aston Villa\n    \n    \n      3\n      2022-12-04\n      Round of 16\n      France\n      Poland\n      3:1\n      0\n      Not in squad\n      Not in squad\n      Lucas Digne\n      Aston Villa\n    \n    \n      4\n      2022-12-10\n      Quarter-Finals\n      England\n      France\n      1:2\n      0\n      Not in squad\n      Not in squad\n      Lucas Digne\n      Aston Villa\n    \n    \n      5\n      2022-12-14\n      Semi-Finals\n      France\n      Morocco\n      2:0\n      0\n      Not in squad\n      Not in squad\n      Lucas Digne\n      Aston Villa\n    \n    \n      6\n      2022-12-18\n      Final\n      Argentina\n      France\n      7:5 on pens\n      0\n      Not in squad\n      Not in squad\n      Lucas Digne\n      Aston Villa\n    \n  \n\n\n\n\nWe can see that France show up the most in the Home team.1 and Away team.1 columns, so we can use this to calculate which team he plays for.\n\nprem_world_cup_minutes_df[prem_world_cup_minutes_df['player_name'] ==\"Lucas Digne\"][['Home team.1', 'Away team.1']].values\n\narray([['France', 'Australia'],\n       ['France', 'Denmark'],\n       ['Tunisia', 'France'],\n       ['France', 'Poland'],\n       ['England', 'France'],\n       ['France', 'Morocco'],\n       ['Argentina', 'France']], dtype=object)\n\n\nSince the team names are spread across two columns, we first convert this into a 2D numpy.array\n\nteams = prem_world_cup_minutes_df[\n    prem_world_cup_minutes_df['player_name'] ==\"Lucas Digne\"\n    ][\n        ['Home team.1', 'Away team.1']\n        ].values.flatten()\nteams\n\narray(['France', 'Australia', 'France', 'Denmark', 'Tunisia', 'France',\n       'France', 'Poland', 'England', 'France', 'France', 'Morocco',\n       'Argentina', 'France'], dtype=object)\n\n\nAnd then flatten it so its a 1D array (like a list) and we can calculate the mode\n\npd.DataFrame(teams).mode().iloc[0,0]\n\n'France'\n\n\nWe can applying this to the full dataset with DataFrame.apply and then merge it with the original dataset so we have an extra country column.\n\nplayer_countries = prem_world_cup_minutes_df.groupby(\"player_name\").apply(lambda df: pd.DataFrame(df[['Home team.1', 'Away team.1']].values.flatten()).mode())\nplayer_countries = player_countries.reset_index().drop(\"level_1\", axis=1).rename({0:'country'}, axis=1)\nprem_world_cup_minutes_df = prem_world_cup_minutes_df.merge(player_countries, how='left', left_on=\"player_name\", right_on=\"player_name\")\nprem_world_cup_minutes_df.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Matchday\n      Home team.1\n      Away team.1\n      Result\n      min_played\n      subbed_on\n      subbed_off\n      player_name\n      club\n      country\n    \n  \n  \n    \n      0\n      2022-11-24\n      Group G\n      Switzerland\n      Cameroon\n      1:0\n      90\n      NaN\n      NaN\n      Manuel Akanji\n      Manchester City\n      Switzerland\n    \n    \n      1\n      2022-11-28\n      Group G\n      Brazil\n      Switzerland\n      1:0\n      90\n      NaN\n      NaN\n      Manuel Akanji\n      Manchester City\n      Switzerland\n    \n    \n      2\n      2022-12-02\n      Group G\n      Serbia\n      Switzerland\n      2:3\n      90\n      NaN\n      NaN\n      Manuel Akanji\n      Manchester City\n      Switzerland\n    \n    \n      3\n      2022-12-06\n      Round of 16\n      Portugal\n      Switzerland\n      6:1\n      90\n      NaN\n      NaN\n      Manuel Akanji\n      Manchester City\n      Switzerland\n    \n    \n      4\n      2022-11-24\n      Group H\n      Portugal\n      Ghana\n      3:2\n      90\n      NaN\n      NaN\n      João Cancelo\n      Manchester City\n      Portugal"
  },
  {
    "objectID": "posts/worldcup-fatigue/index.html#grouping-by-country",
    "href": "posts/worldcup-fatigue/index.html#grouping-by-country",
    "title": "World Cup 2022 Fatigue",
    "section": "4.1 Grouping by country",
    "text": "4.1 Grouping by country\n\n\n\n\n\nThis seems to be committing the sin of way too much data in one plot, there just aren’t enough (significantly) different enough colours to differentiate between the countries, and even if we could - it’d be too much to take in for a visualisation."
  },
  {
    "objectID": "posts/worldcup-fatigue/index.html#grouping-by-tournament-progression",
    "href": "posts/worldcup-fatigue/index.html#grouping-by-tournament-progression",
    "title": "World Cup 2022 Fatigue",
    "section": "4.2 Grouping by tournament progression",
    "text": "4.2 Grouping by tournament progression\nA more meaningful and accessible visualisation is to group together contries that made it to the same stages of the tournament. The progression through the world cup rounds indicates how much rest the players have had and also provides reasons behind the number of minutes: progressing to later rounds is likely to be the cause behind a higher number of minutes played."
  },
  {
    "objectID": "posts/worldcup-fatigue/index.html#finishing-touches",
    "href": "posts/worldcup-fatigue/index.html#finishing-touches",
    "title": "World Cup 2022 Fatigue",
    "section": "4.3 Finishing touches",
    "text": "4.3 Finishing touches\nSince we are exploring the impact of fatigue from the world cup, it would make sense to order the y axis (premier league teams) by league position since this is likely to be affected following the world cup.\n\n\nAdding premier league standings\nfrom fuzzywuzzy import process\nprem_standings = pd.read_html(\"https://www.bbc.co.uk/sport/football/tables\")[0]\nprem_standings = prem_standings.iloc[:-1, [0,2]].rename({'Unnamed: 0':'prem_position'}, axis=1).set_index('Team')\n\n# taken from https://stackoverflow.com/a/56315491\ndef fuzzy_merge(df_1, df_2, key1, key2, threshold=90, limit=2):\n    \"\"\"\n    :param df_1: the left table to join\n    :param df_2: the right table to join\n    :param key1: key column of the left table\n    :param key2: key column of the right table\n    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n    :param limit: the amount of matches that will get returned, these are sorted high to low\n    :return: dataframe with boths keys and matches\n    \"\"\"\n    s = df_2[key2].tolist()\n    \n    m = df_1[key1].apply(lambda x: process.extract(x, s, limit=limit))    \n    df_1['matches'] = m\n    \n    m2 = df_1['matches'].apply(lambda x: ', '.join([i[0] for i in x if i[1] >= threshold]))\n    df_1['matches'] = m2\n    \n    return df_1\ndf = df.reset_index()\nprem_standings = prem_standings.reset_index()\nmerged_standings = fuzzy_merge(df, prem_standings, key1='club', key2='Team', threshold=65, limit=1)\nmerged_standings.loc[merged_standings['club']==\"Manchester United\", 'matches'] = 'Man Utd'\nmerged_standings = merged_standings.merge(prem_standings, how='left', left_on='matches', right_on='Team').drop(['matches', 'Team'], axis=1)\nmerged_standings['prem_position'] = pd.to_numeric(merged_standings['prem_position'])\nmerged_standings.sort_values('prem_position', inplace=True, ascending=False)\n\ndf = merged_standings.drop(\"prem_position\", axis=1).set_index(\"club\")\n\n\nWe can change the colour scheme so it’s more accessible (see colorbrew.org) and somewhat matches the world cup theme, change the fonts, add some spacing and we end up with our final plot:"
  },
  {
    "objectID": "posts/lpool-shot-xg/index.html",
    "href": "posts/lpool-shot-xg/index.html",
    "title": "Does Darwin Nunez only score difficult chances?",
    "section": "",
    "text": "Despite his 10 goals in 23 games (at the time of writing), Darwin Nunez’s finishing has been questioned by some since he joined Liverpool in the summer of 2022. Some of this criticism is due to the inevitable comparison with Manchester City’s summer signing of Erling Haaland who has taken the league by storm with his 27 goals in 24 games so far.\nIt feels as though Nunez loses his composure in front of goals with easier chances but looks calmer with more difficult chances when he isn’t expected to score. To put this “feeling” to the test, we can look at the difficulty of chances that are scored vs ones that are missed with the help of the expected goals (xG) metric.\nAny thoughts, comments or ideas? Please let me know here!"
  },
  {
    "objectID": "posts/lpool-shot-xg/index.html#data-source",
    "href": "posts/lpool-shot-xg/index.html#data-source",
    "title": "Does Darwin Nunez only score difficult chances?",
    "section": "1 Data source",
    "text": "1 Data source\nTo help make a judgement on the quality of chances missed vs scored, xG data was required at the finest granularity level: each individual shot by a player.\nYou can read about how data was scraped from fbref while still respecting the API limits here (coming soon!).\n\n\n\n\n\n\n  \n    \n      \n      Minute\n      Player\n      xG\n      Outcome\n      is_goal\n    \n  \n  \n    \n      2022-10-04\n      12\n      Luis Díaz\n      0.07\n      Off Target\n      False\n    \n    \n      2021-12-01\n      3\n      Mohamed Salah\n      0.34\n      Off Target\n      False\n    \n    \n      2021-11-27\n      2\n      Diogo Jota\n      0.62\n      Goal\n      True\n    \n    \n      2021-09-19\n      25\n      Cody Gakpo\n      0.04\n      Off Target\n      False\n    \n    \n      2021-10-02\n      5.0\n      Luis Díaz\n      0.14\n      Blocked\n      False"
  },
  {
    "objectID": "posts/lpool-shot-xg/index.html#visualising-all-shots",
    "href": "posts/lpool-shot-xg/index.html#visualising-all-shots",
    "title": "Does Darwin Nunez only score difficult chances?",
    "section": "2 Visualising all shots",
    "text": "2 Visualising all shots\nFinding a balance between visualising all the individual shot data points and not making the plot too busy was the key goal of this visualisation. By using text annotations in both the y axis and the mean lines - I think a decent tradeoff was achieved with seaborn’s stripplot.\nThis was done fairly manually by looping through the tick labels to get x,y positions to place the annotations:\n\ntick_label_order = [label._text for label in ax.get_yticklabels()]\n\nfor idx, label in enumerate(tick_label_order):\n        num_shots = len(concat_shots[concat_shots['Player']==label])\n        num_goals = len(player_goals[player_goals['Player']==label])\n        shots_per_goal = num_shots/num_goals\n        ax.annotate(\n            f\"shots: {num_shots}\\ngoals: {num_goals}\\nshots/goal: {shots_per_goal:.2f}\", \n            xy=(-0.05,idx + 0.33), \n            xycoords='data', \n            annotation_clip=False, \n            horizontalalignment='right'\n        )\n\nThe mean xG lines were added in a similar way, using the tick_label_order index to find the corresponding mean values:\n\nmean_goal_lines = [\n    (idx,mean_goal_xg[mean_goal_xg['Player']==tick]['xG'].squeeze()) for idx, tick in enumerate(tick_label_order)\n    ]\n# add avg xG lines\nnum_y_labels = len(mean_goal_lines)\n\nfor y,x in mean_goal_lines:\n    y_loc = 1- (y/num_y_labels)\n    y_width = 1/num_y_labels\n    line_height = 1/(num_y_labels+1)\n\n    ax.axvline(x,ymin = y_loc-line_height, ymax = y_loc - y_width + line_height, color='#1f77b4', linewidth = 4, alpha=0.5)\n    offset = 10\n    ax.annotate(f\"{x:.2f}\", (x,y), xytext=(offset/2, 2*offset), textcoords='offset points')\n\n# repeat for mean xg of shots missed"
  },
  {
    "objectID": "posts/lpool-shot-xg/index.html#adding-contextual-data",
    "href": "posts/lpool-shot-xg/index.html#adding-contextual-data",
    "title": "Does Darwin Nunez only score difficult chances?",
    "section": "3 Adding contextual data",
    "text": "3 Adding contextual data\nThere are a few interesting data points in the plot above, so it would be cool to be able to search up the goal or missed shot - e.g. the Cody Gakpo goal with an xG of ~0.01 or the chance Luis Diaz missed with an xG of ~0.95.\nThe static plot is already at the limits of how noisy I would want to get, but we can use an interactive plotting tool like plotly to add more data without ruining the simplicity of the plot.\nFirst we can add the match data:\n\nconcat_shots = concat_shots.reset_index().rename({'index':'date'}, axis=1)\nconcat_shots.sample(5)\n\n\n\n\n\n  \n    \n      \n      date\n      Minute\n      Player\n      xG\n      Outcome\n      Venue\n      Result\n      Squad\n      Opponent\n      is_goal\n    \n  \n  \n    \n      854\n      2022-02-23\n      15\n      Mohamed Salah\n      0.79\n      Goal\n      Home\n      W 6–0\n      Liverpool\n      Leeds United\n      True\n    \n    \n      38\n      2021-04-24\n      15\n      Cody Gakpo\n      0.03\n      Woodwork\n      Home\n      W 1–0\n      PSV Eindhoven\n      Groningen\n      False\n    \n    \n      844\n      2022-02-16\n      8.0\n      Mohamed Salah\n      0.09\n      Blocked\n      Away\n      W 2–0\n      eng Liverpool\n      it Inter\n      False\n    \n    \n      115\n      2022-03-10\n      62\n      Cody Gakpo\n      0.79\n      Saved\n      Home\n      D 4–4\n      nl PSV Eindhoven\n      dk FC Copenhagen\n      False\n    \n    \n      1313\n      2022-03-11\n      78\n      Darwin Núñez\n      0.03\n      Saved\n      Home\n      D 1–1\n      Benfica\n      Vizela\n      False\n    \n  \n\n\n\n\nThen plot the data using plotly.express.stripplot:"
  },
  {
    "objectID": "posts/lpool-shot-xg/index.html#is-liverpool-is-the-problem",
    "href": "posts/lpool-shot-xg/index.html#is-liverpool-is-the-problem",
    "title": "Does Darwin Nunez only score difficult chances?",
    "section": "4 Is Liverpool is the problem?",
    "text": "4 Is Liverpool is the problem?\nGiven Liverpool’s run of form this season, and the extra pressure of moving to a new country, there’s plenty of cause for Nunez to be fluffing his shots more than usual this season. Now that we have match data, we can easily split the shots at Liverpool and previous clubs:\n\nconcat_shots['Squad'] = concat_shots['Squad'].apply(lambda x:x.split(\" \")[-1])\nnunez_shots = concat_shots[concat_shots['Player'] == 'Darwin Núñez']\n#remove uruguay\nnunez_shots.drop(nunez_shots[nunez_shots['Squad'] =='Uruguay'].index, inplace=True)\nnunez_shots.sample(5)\n\n\n\n\n\n  \n    \n      \n      date\n      Minute\n      Player\n      xG\n      Outcome\n      Venue\n      Result\n      Squad\n      Opponent\n      is_goal\n    \n  \n  \n    \n      1212\n      2020-12-10\n      61\n      Darwin Núñez\n      0.08\n      Woodwork\n      Away\n      D 2–2\n      Benfica\n      be Standard Liège\n      False\n    \n    \n      1335\n      2022-04-23\n      9\n      Darwin Núñez\n      0.03\n      Off Target\n      Home\n      D 0–0\n      Benfica\n      Famalicão\n      False\n    \n    \n      1399\n      2022-11-06\n      82\n      Darwin Núñez\n      0.16\n      Woodwork\n      Away\n      W 2–1\n      Liverpool\n      Tottenham\n      False\n    \n    \n      1186\n      2020-10-18\n      44\n      Darwin Núñez\n      0.06\n      Blocked\n      Away\n      W 3–0\n      Benfica\n      Rio Ave\n      False\n    \n    \n      1332\n      2022-04-13\n      83\n      Darwin Núñez\n      0.03\n      Saved\n      Away\n      D 3–3\n      Benfica\n      eng Liverpool\n      False\n    \n  \n\n\n\n\nThe sample size for Benfica is a lot larger (60 games at Benfica vs 19 games at Liverpool), but from the plot below we can see that while Nunez’s shots/goal is almost double what it was at Benfica - there’s no difference in the average xG of the shots he’s been missing. The average xG for goals is a lot higher at Benfica suggesting that Nunez is actually scoring more difficult chances at Liverpool - so atleast that part of the initial feeling was true.\n\n\n\n\n\nTo test out whether Nunez is actually taking more shots at Liverpool we can just look at the number of shots per game:\n\nnunez_lpool_shots = nunez_shots[nunez_shots['Squad']=='Liverpool']\nnunez_benfica_shots = nunez_shots[nunez_shots['Squad']!='Liverpool']\n\nnum_lpool_games = len(nunez_lpool_shots['date'].unique())\nnum_benfica_games = len(nunez_benfica_shots['date'].unique())\n\n\nprint(\n    f\"shots/game at Liverpool: {len(nunez_lpool_shots)/num_lpool_games:.2f}\\n\"\n    f\"shots/game at Benfica: {len(nunez_benfica_shots)/num_benfica_games:.2f}\"\n)\n\nshots/game at Liverpool: 3.79\nshots/game at Benfica: 2.83\n\n\nAnd they seem to be lower quality too:\n\nprint(\n    f\"avg shot xG at Liverpool: {nunez_lpool_shots['xG'].mean():.2f}\\n\"\n    f\"avg shot xG at Benfica: {nunez_benfica_shots['xG'].mean():.2f}\"\n)\n\navg shot xG at Liverpool: 0.15\navg shot xG at Benfica: 0.20"
  },
  {
    "objectID": "posts/pytest-sql/index.html",
    "href": "posts/pytest-sql/index.html",
    "title": "Pytest and SQL",
    "section": "",
    "text": "The source code for the demo referenced in this post can be found here. You can jump to the actual testing part here (skip all the intro stuff)\nAny thoughts, comments or ideas? Please let me know here!"
  },
  {
    "objectID": "posts/pytest-sql/index.html#instantiate-the-test-database",
    "href": "posts/pytest-sql/index.html#instantiate-the-test-database",
    "title": "Pytest and SQL",
    "section": "5.1 Instantiate the test database",
    "text": "5.1 Instantiate the test database\nBefore doing any querying or testing, we need to start a database instance - docker makes this quite painless.\nWe’ll define the image in a Dockerfile and add some seed data that we’ll need for our query. We could’ve chosen to add this data in our tests or fixtures, but since this is reference data and is not specific to a test, we can add it in during the image build.\nThe seed data script creates the table and adds the reference data:\nCREATE TABLE public.food_ref (\n    id varchar NOT NULL,\n    \"name\" varchar NULL,\n    \"kcal/unit\" int4 NULL,\n    unit_of_measure varchar NULL,\n    CONSTRAINT food_ref_pkey PRIMARY KEY (id)\n);\n\nINSERT INTO\n    public.food_ref (id, \"name\", \"kcal/unit\", unit_of_measure)\nVALUES\n('a', 'bread', 10, 'g')\n-- we can add more values here;\nThe postgres docker image will run any .sql (or .sh) scripts in docker-entrypoint-initdb.d/ so we can copy our seed script there:\nFROM postgres:10-alpine\nENV POSTGRES_USER=postgres\nENV POSTGRES_PASSWORD=password\nEXPOSE 5432\n# seed database with food reference data\nCOPY ./seed_scripts/food_ref.sql /docker-entrypoint-initdb.d/"
  },
  {
    "objectID": "posts/pytest-sql/index.html#setup-fixtures",
    "href": "posts/pytest-sql/index.html#setup-fixtures",
    "title": "Pytest and SQL",
    "section": "5.2 Setup fixtures",
    "text": "5.2 Setup fixtures\nWe can define a session-scope fixture which uses the docker library to build the image and run the container for our tests. Using yield here means that once the tests have run, control is returned back to this fixture and we can remove the container and image so we return to a clean state.\n@pytest.fixture(scope=\"session\")\ndef postgres_container():\n    \"\"\"start docker instance of postgres\"\"\"\n    client = docker.from_env()\n\n    client.images.build(\n        path=TEST_DB_DOCKERFILE_PATH, tag=\"test_postgres\", nocache=True\n    )\n\n    try:\n        # create container\n        db_container = client.containers.run(\n            \"test_postgres\",\n            name=\"test_db\",\n            detach=True,\n            ports={5432: 5432},\n        )\n\n    except APIError:\n        # docker api returns an error sincewe already have a container\n        # remove existing container and make a new one\n        db_container = client.containers.get(\"test_db\")\n        db_container.remove()  # type:ignore\n        db_container = client.containers.run(\n            \"test_postgres\",\n            name=\"test_db\",\n            detach=True,\n            ports={5432: 5432},\n        )\n    # return to calling function so we can use the container\n    yield\n    # calling function is done so we can stop and remove the container\n    db_container.stop()  # type:ignore\n    db_container.remove()  # type:ignore\nThe only other setup we need is to create a fixture for a session that runs on the test database container above, this will allow us to define some steps to connect, run our tests and then cleanup the database to remove all the data we added for our test:\n@pytest.fixture\ndef test_session(postgres_container) -> Iterator[Session]:\n    \"\"\"create db session and cleanup data after each test\"\"\"\n    # testing connection string\n    DOCKER_USERNAME = \"postgres\"\n    DOCKER_PASSWORD = \"password\"\n    DOCKER_HOSTNAME = \"localhost:5432\"\n    DOCKER_DB_NAME = \"postgres\"\n\n    sql_url = (\n        f\"postgresql://{DOCKER_USERNAME}:{DOCKER_PASSWORD}@\"\n        f\"{DOCKER_HOSTNAME}/{DOCKER_DB_NAME}\"\n    )\n    engine = create_engine(sql_url, pool_pre_ping=True)\n\n    # wait until db is ready\n    MAX_RETRIES = 100\n    #  max value for backoff time\n    MAX_RETRY_SLEEP_SEC = 2\n\n    num_retries = 0\n    try:\n        while num_retries < MAX_RETRIES:\n            num_retries += 1\n            # keep increasing back off time until MAX_RETRY_SLEEP_SEC\n            # using (truncated) exponential backoff\n            sleep_time_ms = min(\n                [MAX_RETRY_SLEEP_SEC * 1000, 100 * num_retries]\n            )\n            try:\n                # check if db is ready with simple query\n                engine.execute(\"SELECT 1\")\n            except OperationalError:\n                # db is still starting up, continue loop and retry\n                sleep(sleep_time_ms / 1000)\n                continue\n            # db has started - lets break out of loop\n            break\n    except OperationalError:\n        raise Exception(\"Couldn't connect to Test Postgres Docker Instance!\")\n\n    # create all tables registered to our declarative base class\n    Base.metadata.create_all(engine)\n    with Session(engine) as session:\n        yield session\n        session.commit()\n\n    # clean up our db\n    Base.metadata.drop_all(engine)\nThe logic in ln16 - ln38 ensures that we do not try to create our session before the database in the container is ready. The client.containers.run function in the postgres_container fixture returns before the database is ready since the container has already started successfully."
  },
  {
    "objectID": "posts/pytest-sql/index.html#the-payoff",
    "href": "posts/pytest-sql/index.html#the-payoff",
    "title": "Pytest and SQL",
    "section": "5.3 The payoff",
    "text": "5.3 The payoff\nFinally we can actually test our query:\ndef test_get_net_calories(test_session: Session):\n    food_cons = FoodConsumption(\n        food_id=\"a\", qty=\"100\", date=datetime.date.today()\n    )\n    bike_session = Bike(\n        date=datetime.date.today(), speed_mph=5, duration_min=20\n    )\n    with test_session:\n        test_session.add_all([food_cons, bike_session])\n        test_session.commit()\n\n    result = get_total_net_calories(test_session)\n\n    assert result == 980\nBreaking down what happens when we run our test:\n\nln1 we pass in test_session to our test which call the test_session fixture above\nThe test_session fixture call the postgres_container fixture\nThe db image is built, seeded with reference data and the container is started\nA session is created after connecting to the db container\nln2 - ln10 we add some data specific to our test (ARRANGE)\nln12 we run our function under test and pass it the test_session (ACT)\nln14 we ensure the data is what we expect (ASSERT)\n\nReturn to the test_session fixture and delete all data in the db (i.e. undo step 5.)\nReturn to the postgres_container fixture and stop the container and remove it.\n???\nPROFIT!!!"
  },
  {
    "objectID": "posts/CL19-strangers/avail-heatmaps.html",
    "href": "posts/CL19-strangers/avail-heatmaps.html",
    "title": "The Mystery of 2019 Champions League Final",
    "section": "",
    "text": "This blog was inspired by this reddit post.\nYou can skip to the final visualisation here.\nOn the night of June 1st in 2019, Liverpool would go on to win their 6th Champions League, their first since the dramatic final in 2005 and also the first major trophy won under Jurgen Klopp who joined the club in 2015. That night was significant for many reasons but a strange statistic caught my attention:\n\nThe 11 players that started in the Champions league final had never started a game together before, and has never started again since.\n\nThe rest of this article will explore the data around this statistic and uncover why this team has only ever started together once.\n\n\n\nStarting 11 for the final\n\n\nThis starting lineup is remembered by many fans as the defining team of that era and many of the comments from the reddit post capture why the statistic above is so unintuitive.\n\n\n\nComments on the stat from reddit\n\n\nSo how can this be possible? To try and uncover the mystery we can start by looking at the data from the games played during that season and extract the starting 11 from each game.\n\n1 Data Collection\nThere are many free resources for football data, but transfermarket seemed to be the best option to minimise the post-processing needed for this analysis. The code used to scrape this data can be found here - the key libraries used were pandas, beautifulsoup and requests.\n\n\nData collection and processing libraries\nimport sys\n\n# filter pandas warnings before importing\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pandas as pd\n\n# haven't published library to pypi yet so add local folder to path\n# see https://github.com/ismailmo1/football-data-viz/tree/main/transfermarket\n\nmodule_path = '/home/ismail/projects/football-data-viz/transfermarket/'\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nfrom transfermarket import leagues, players, teams\n\n\n\nprem_clubs_18 = leagues.get_prem_club_list(season=\"2018\")\nlpool_19_players = teams.get_players(team_url=prem_clubs_18['Liverpool FC'])\nlpool_19_players.keys()\n\ndict_keys(['Virgil van Dijk', 'Adam Lallana', 'Sheyi Ojo', 'Andrew Robertson', 'Alisson', 'Georginio Wijnaldum', 'Roberto Firmino', 'Dominic Solanke', 'Rhian Brewster', 'Ádám Bogdán', 'Loris Karius', 'Harry Wilson', 'Alex Oxlade-Chamberlain', 'Kamil Grabara', 'Jordan Henderson', 'Daniel Sturridge', 'Divock Origi', 'Nathaniel Clyne', 'Ben Woodburn', 'Danny Ings', 'Sadio Mané', 'Joel Matip', 'Isaac Christie-Davies', 'James Milner', 'Caoimhín Kelleher', 'Joe Gomez', 'Simon Mignolet', 'Fabinho', 'Marko Grujić', 'Ki-Jana Hoever', 'Trent Alexander-Arnold', 'Rafael Camacho', 'Xherdan Shaqiri', 'Dejan Lovren', 'Curtis Jones', 'Alberto Moreno', 'Connor Randall', 'Naby Keïta', 'Mohamed Salah'])\n\n\nWe’re only interested in the starting 11 from the final so the we can filter out the other players from the squad:\n\ncl_19_lineup = ['Trent Alexander-Arnold',\n                'Georginio Wijnaldum',\n                'Fabinho',\n                'Jordan Henderson',\n                'Roberto Firmino',\n                'Mohamed Salah',\n                'Joel Matip',\n                'Virgil van Dijk',\n                'Sadio Mané',\n                'Andrew Robertson',\n                'Alisson']\n\ncl_19_lineup_urls = {player:\"https://www.transfermarkt.com\" + url for player,\n                        url in lpool_19_players.items() if player in cl_19_lineup}\n\nFor each of these players we can grab their availability for each game.\nget_player_availability scrapes the statistics page for each player and determines which availability category a player falls into for that game.\nThe categories are:\n\nInjured\nNot in squad\nBench\nPlayed (sub)\nPlayed (starter)\n\n\navail_18_19 = []\n\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2018\")\n    avail_18_19.append(avail_df)\n\n\n\n2 Data Processing\nMost of the data processing is done in a separate module (see code here) but for this specific analysis we want to ensure we are only looking at data related to Liverpool fixtures, since the player availability data will also include international games for their country.\n\nstrangers= pd.concat(avail_18_19)\nliverpool_fix = teams.get_team_fixtures(prem_clubs_18[\"Liverpool FC\"], \"2018\")\nfixtures_num_starters = strangers.T.join(liverpool_fix, how=\"right\")\nheatmap_data = fixtures_num_starters.iloc[:, :11].T\n\nThere are also cases where the availability doesn’t fall into any of the categories above, so in that case we assume that they weren’t in the squad. This usually occurs when the player is suspended since the web-scraping logic doesn’t account for that (yet) - but there could also be other cases where transfermarket’s description doesn’t meet any of the logic defined in the web-scraping functions. We’ll see some more examples later where there are rooms for improvement in the web-scraper and the visualisations help identify these.\n\n# assumption: NaN value means they were likely suspended\n# either way they werent in the squad so makes sense \n# to assign that (to me atleast!) \nfixtures_num_starters.fillna(1, inplace=True)\nfixtures_num_starters = fixtures_num_starters.iloc[:,:11]\n\nThe number of players in the starting lineup can be calculated from the availability category they were put into: we can take a sum of all the instances of “Played (starter)” for each game.\n\n# add count\nfixtures_num_starters['strangers_count'] = (fixtures_num_starters == 4 ).sum(axis=1)\nfixtures_num_starters.reset_index(inplace=True, drop=True)\n\nThis is what our processed data looks like (showing first 5 rows only)\n\n\n\n\n\n\n  \n    \n      \n      virgil-van-dijk\n      andrew-robertson\n      alisson\n      georginio-wijnaldum\n      roberto-firmino\n      jordan-henderson\n      sadio-mane\n      joel-matip\n      fabinho\n      trent-alexander-arnold\n      mohamed-salah\n      strangers_count\n    \n  \n  \n    \n      0\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      4.0\n      1.0\n      2.0\n      4.0\n      4.0\n      8\n    \n    \n      1\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      4.0\n      2.0\n      1.0\n      4.0\n      4.0\n      8\n    \n    \n      2\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      4.0\n      3.0\n      1.0\n      4.0\n      4.0\n      8\n    \n    \n      3\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      1.0\n      4.0\n      4.0\n      9\n    \n    \n      4\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      4.0\n      3.0\n      2.0\n      4.0\n      4.0\n      8\n    \n  \n\n\n\n\n\n\n3 Initial Visualisation\nNow for the fun part! Since our data is 2 dimensional : players and dates, a heatmap seems like it would be a good way to to visualise the availability of each individual player over the course of the season, and a bar plot to keep count of how many of them started the game:\n\n\nVisualisation libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nmatplotlib.rcParams['font.family'] = 'JetBrains Mono'\nmatplotlib.rcParams['font.size'] = '20'\n\n\nWhile this plot still needs a lot of work and cleanup - it’s a nice way to visualise the data - we can see that the final column in the heatmap is the only time when all boxes are green: of course this is the night of that champions league final.\nThere are quite a few things to fix in the visualisation here:\n\nAdd titles and labels to axes\nChange y tick labels in player count bar plot to include max (11)\nReplace color bar with text instead of numbers to map availability level categories\nImprove color scheme to make it more accessible\nAlign the bars in the bar plot so it is aligned with the heatmap squares - currently it is offset\n\n\nfig,(ax1,ax2) = plt.subplots(figsize=(30,10), nrows=2, sharex=True, height_ratios=[1, 4], constrained_layout=True)\n\nbar_plot = ax1.bar(fixtures_num_starters.index, fixtures_num_starters[\"strangers_count\"])\nplot = sns.heatmap(fixtures_num_starters.iloc[:, :-1].T, ax=ax2, cmap=sns.color_palette(\"RdYlGn\"),linewidths=0.5, linecolor='lightgray')\n\n\n\n\nBefore we address the visualisation adjustments, lets grab some data for the previous season (2017/2018) so we can add some more context around the statistic:\n\n\n4 More Data Processing\n\nliverpool_fix_17_18 = teams.get_team_fixtures(prem_clubs_18[\"Liverpool FC\"], \"2017\")\n\n\navail_17_18 = []\n\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2017\", add_match_result=True)\n    # add liverpool fixture data\n    avail_df = avail_df.T.join(liverpool_fix_17_18, how=\"right\", rsuffix=\"_lpool\")\n    avail_17_18.append(avail_df.T)\n\nMore data, more problems - we now need to account for when players didn’t play for Liverpool - e.g. Virgil Van Dijk only join in January 2018 and was playing for Southampton during the first half of the season:\n\nvvd_idx = [idx for idx, df in enumerate(avail_17_18) if 'virgil' in df.index[0]][0]\n\n\navail_17_18[vvd_idx].iloc[:5, :5].dropna(axis=1)\n\n\n\n\n\n  \n    \n      Date\n      2017-08-12\n      2017-08-19\n      2017-08-23\n    \n  \n  \n    \n      virgil-van-dijk\n      0\n      0\n      0\n    \n    \n      Home team.1\n      Southampton (11.)\n      Southampton (11.)\n      Southampton\n    \n    \n      Away team.1\n      Swansea (12.)\n      West Ham (20.)\n      Wolves\n    \n    \n      Result\n      0:0\n      3:2\n      0:2\n    \n    \n      Matchday\n      1\n      2\n      Qualifying Round 2nd leg\n    \n  \n\n\n\n\nWe can account for this by adding another category to our availability levels:\n\ndef is_liverpool(rw):\n    try:\n        if 'Liverpool' in rw['Home team.1'] or 'Liverpool' in rw['Away team.1']:\n            return True\n        else:\n            return False\n    except TypeError:\n        return False \nvvd = avail_17_18[vvd_idx].T\nvvd['is_lpool'] = vvd.apply(is_liverpool, axis=1)\n\ndef add_diff_team_availability(rw):\n    \"\"\"add -1 if player isn't playing a liverpool game\"\"\"\n    if rw['is_lpool'] == False:\n        return -1\n    else:\n        # availability col\n        return rw[0]\n\nvvd.iloc[:,0] = vvd.apply(add_diff_team_availability, axis=1)\n\nThis heuristic doesn’t work when they play against Liverpool for their old club - but we can just fix that manually since it’ll be easy to spot (one game for Liverpool surrounded by loads of games for Southampton will stick out like a sore thumb)\n\nvvd.sample(5).iloc[:, :3].dropna()\n\n\n\n\n\n  \n    \n      \n      virgil-van-dijk\n      Home team.1\n      Away team.1\n    \n    \n      Date\n      \n      \n      \n    \n  \n  \n    \n      2018-05-06\n      4\n      Chelsea (5.)\n      Liverpool (4.)\n    \n    \n      2018-02-11\n      4\n      Southampton (15.)\n      Liverpool (3.)\n    \n    \n      2018-04-21\n      4\n      West Brom (20.)\n      Liverpool (3.)\n    \n    \n      2017-11-29\n      -1\n      Man City (1.)\n      Southampton (10.)\n    \n    \n      2018-02-04\n      4\n      Liverpool (3.)\n      Spurs (5.)\n    \n  \n\n\n\n\n\n\nLet’s now grab data for the seasons before and after (17/18 and 19/20, 20/21 and 21/22)\n# should have refactored the data collection/processing loop into a function\n\navail_17_18 = []\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2017\", add_match_result=True)\n    # add liverpool fixture data\n    transposed_df = avail_df.T\n    transposed_df= transposed_df.join(liverpool_fix_17_18, how=\"right\", rsuffix=\"_lpool\")\n\n    transposed_df['is_lpool'] = transposed_df.apply(is_liverpool, axis=1)\n    transposed_df.iloc[:,0] = transposed_df.apply(add_diff_team_availability, axis=1)\n    transposed_df = pd.DataFrame(transposed_df.iloc[:,0]).T\n    avail_17_18.append(transposed_df)\n    \navail_17_18_all = pd.concat(avail_17_18)\navail_17_18_all.fillna(1, inplace=True)\n\n# add count\nlong_avail_17_18_df = avail_17_18_all.T\nlong_avail_17_18_df['strangers_count'] = (long_avail_17_18_df == 4 ).sum(axis=1)\nlong_avail_17_18_df.reset_index(inplace=True, drop=True)\n#| echo: false\nliverpool_fix_19_20 = teams.get_team_fixtures(prem_clubs_18[\"Liverpool FC\"], \"2019\")\navail_19_20 = []\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2019\", add_match_result=True)\n    # add liverpool fixture data\n    transposed_df = avail_df.T\n    transposed_df= transposed_df.join(liverpool_fix_19_20, how=\"right\", rsuffix=\"_lpool\")\n\n    transposed_df['is_lpool'] = transposed_df.apply(is_liverpool, axis=1)\n    transposed_df.iloc[:,0] = transposed_df.apply(add_diff_team_availability, axis=1)\n    transposed_df = pd.DataFrame(transposed_df.iloc[:,0]).T\n    avail_19_20.append(transposed_df)\n    \navail_19_20_all = pd.concat(avail_19_20)\navail_19_20_all.fillna(1, inplace=True)\n\n# add count\nlong_avail_19_20_df = avail_19_20_all.T\nlong_avail_19_20_df['strangers_count'] = (long_avail_19_20_df == 4 ).sum(axis=1)\nlong_avail_19_20_df.reset_index(inplace=True, drop=True)\n\n#add 2020/2021\n\nliverpool_fix_20_21 = teams.get_team_fixtures(prem_clubs_18[\"Liverpool FC\"], \"2020\")\navail_20_21 = []\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2020\", add_match_result=True)\n    # add liverpool fixture data\n    transposed_df = avail_df.T\n    transposed_df= transposed_df.join(liverpool_fix_20_21, how=\"right\", rsuffix=\"_lpool\")\n\n    transposed_df['is_lpool'] = transposed_df.apply(is_liverpool, axis=1)\n    transposed_df.iloc[:,0] = transposed_df.apply(add_diff_team_availability, axis=1)\n    transposed_df = pd.DataFrame(transposed_df.iloc[:,0]).T\n    avail_20_21.append(transposed_df)\n    \navail_20_21_all = pd.concat(avail_20_21)\navail_20_21_all.fillna(1, inplace=True)\n\n# add count\nlong_avail_20_21_df = avail_20_21_all.T\nlong_avail_20_21_df['strangers_count'] = (long_avail_20_21_df == 4 ).sum(axis=1)\nlong_avail_20_21_df.reset_index(inplace=True, drop=True)\n#add 2021/2022\n\nliverpool_fix_21_22 = teams.get_team_fixtures(prem_clubs_18[\"Liverpool FC\"], \"2021\")\navail_21_22 = []\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2021\", add_match_result=True)\n    # add liverpool fixture data\n    transposed_df = avail_df.T\n    transposed_df= transposed_df.join(liverpool_fix_21_22, how=\"right\", rsuffix=\"_lpool\")\n\n    transposed_df['is_lpool'] = transposed_df.apply(is_liverpool, axis=1)\n    transposed_df.iloc[:,0] = transposed_df.apply(add_diff_team_availability, axis=1)\n    transposed_df = pd.DataFrame(transposed_df.iloc[:,0]).T\n    avail_21_22.append(transposed_df)\n    \navail_21_22_all = pd.concat(avail_21_22)\navail_21_22_all.fillna(1, inplace=True)\n\n# add count\nlong_avail_21_22_df = avail_21_22_all.T\nlong_avail_21_22_df['strangers_count'] = (long_avail_21_22_df == 4 ).sum(axis=1)\nlong_avail_21_22_df.reset_index(inplace=True, drop=True)\n\n# concatenate all our data \nall_fix_dates = [*avail_17_18_all.columns, *liverpool_fix.index,*avail_19_20_all.columns, *avail_20_21_all.columns,*avail_21_22_all.columns]\nall_avail = pd.concat([long_avail_17_18_df, fixtures_num_starters, long_avail_19_20_df, long_avail_20_21_df,long_avail_21_22_df])\n\n# reset index so we dont have to deal with dodgy dates in axes\nall_avail.reset_index(inplace=True, drop=True)\nall_avail['date'] = all_fix_dates\nall_avail.tail()\n\n\n\n\n\n\n  \n    \n      \n      virgil-van-dijk\n      andrew-robertson\n      alisson\n      georginio-wijnaldum\n      roberto-firmino\n      jordan-henderson\n      sadio-mane\n      joel-matip\n      fabinho\n      trent-alexander-arnold\n      mohamed-salah\n      strangers_count\n      date\n    \n  \n  \n    \n      282\n      4.0\n      1.0\n      4.0\n      -1.0\n      2.0\n      3.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      6\n      2022-05-10\n    \n    \n      283\n      4.0\n      4.0\n      4.0\n      -1.0\n      3.0\n      4.0\n      4.0\n      3.0\n      0.0\n      4.0\n      4.0\n      7\n      2022-05-14\n    \n    \n      284\n      0.0\n      2.0\n      4.0\n      -1.0\n      4.0\n      3.0\n      1.0\n      4.0\n      0.0\n      1.0\n      1.0\n      3\n      2022-05-17\n    \n    \n      285\n      2.0\n      4.0\n      4.0\n      -1.0\n      3.0\n      4.0\n      4.0\n      4.0\n      0.0\n      4.0\n      3.0\n      6\n      2022-05-22\n    \n    \n      286\n      4.0\n      4.0\n      4.0\n      -1.0\n      3.0\n      4.0\n      4.0\n      2.0\n      4.0\n      4.0\n      4.0\n      8\n      2022-05-28\n    \n  \n\n\n\n\nTo address some of the ugliness from the initial visualisation we can make some changes in the visualisation setup (expand code and see comments):\n\n\nVisualisation setup\navailability_levels_map = {\n                        'Not at Liverpool' : -1,\n                        'Injured':0, \n                        'Not in squad':1,\n                        'Bench':2, \n                        'Played (sub)':3,\n                        'Played (starter)':4 \n                        }\nfig,(ax1,ax2) = plt.subplots(figsize=(30,10), nrows=2, sharex=True, height_ratios=[1, 4], constrained_layout=True)\n\n# align bars to edge so its aligned with the heatmap\nbar_plot = ax1.bar(all_avail.index, all_avail[\"strangers_count\"], align=\"edge\")\n#  remove x axis label (since we share with heatmap)\n_ = ax1.set_xlabel(\"\")\n_ = ax1.tick_params(length=0)\n# add bar graph y label and only add tick label to max #players\n_ = ax1.set_ylabel(\"#players starting\",rotation=\"horizontal\", ha=\"right\" )\n_ = ax1.yaxis.set_major_locator(matplotlib.ticker.FixedLocator([11]))\n_ = ax1.set_title(\"When stars align: The Mythical 2019 CL Final Starting 11\")\n\nnum_levels = len(availability_levels_map.keys())\nplot = sns.heatmap(all_avail.iloc[:, :-2].T, ax=ax2, cmap=sns.color_palette(\"RdYlGn\",num_levels),linewidths=0.5, linecolor='lightgray')\n\n# map colorbar labels with availability levels\ncolorbar = plot.collections[0].colorbar\ntotal_bar_height = colorbar.vmax - colorbar.vmin \nlvl_bar_height = total_bar_height/num_levels\n\ncolorbar.set_ticks([colorbar.vmin + (lvl_bar_height * (0.5 + i)) for i in range(num_levels)])\ncolorbar.set_ticklabels([*availability_levels_map.keys()])\n\n# gotta love dates - format them so they look cleaner\naxs_fmtd = [all_avail.loc[int(i.get_text()), 'date'].strftime(\"%d %b\") for i in ax2.get_xticklabels()]\n_ = ax2.set_xticklabels(axs_fmtd, rotation=\"vertical\")\n_ = ax2.set_xlabel(\"\")\n_ = ax2.tick_params(length=0)\n_ = plt.gcf().text(0.9, 0, \"ismailmo.com\", fontsize=12, fontdict={'color':  'blue'} )\n\n\n\n\n\nThere are some anomalies where players are marked as “Not at liverpool” even though they definitely were (see Trent, Mane and salah’s red lines)\nMane and Salah were playing eachother at the AFCON final - so technically the graph is correct :)\n\n\n\n\n\n\n  \n    \n      \n      sadio-mane\n      date\n    \n  \n  \n    \n      258\n      -1.0\n      2022-02-06\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      sadio-mane\n      date\n    \n  \n  \n    \n      258\n      -1.0\n      2022-02-06\n    \n  \n\n\n\n\nFor Trent, the England U21 team were playing at the same time (which Trent used to play for) so our logic that adjusted for games played for other clubs (e.g. Van Dijk for Southampton) picked up this game- since this is a rare edge case I’ll just overwrite it manually\n\n\n\n\n\n\n  \n    \n      Date\n      2018-05-26\n      2018-05-26\n    \n  \n  \n    \n      trent-alexander-arnold\n      4\n      1\n    \n    \n      Home team.1\n      Real Madrid\n      England U21\n    \n    \n      Away team.1\n      Liverpool\n      China U20\n    \n    \n      Result\n      3:1\n      2:1\n    \n  \n\n\n\n\n\n\nManually adjusting outlier for Trent\ntrent_outlier = all_avail['date']==datetime.datetime(2018,5,26)\nall_avail.loc[trent_outlier,\"trent-alexander-arnold\"] = 4\n\n\nThe visualisation above shows that the full squad were only playing for Liverpool FC during 2018/19 - 2021-22 (see the blocks of red at the start and end of the heatmap) - we can remove the other seasons (we’ll keep a few rows to highlight their departure/arrival at Liverpool).\nAnother key change is choosing a better colours palette - ColorBrewer is a great resource for finding accessible color schemes based on the research of Dr Cynthia Brewer\n\n\nNarrowing down the date range\ncl_19_lineup_at_lpool_avail_df = all_avail[(all_avail.loc[:,'date']>datetime.datetime(2018,5,10)) &( all_avail.loc[:,'date']<datetime.datetime(2021,8,30))]\ncl_19_lineup_at_lpool_avail_df.reset_index(inplace=True, drop=True)\n\n\n\n\nFinal visualisation setup\n# order players by position\nheatmap_data = cl_19_lineup_at_lpool_avail_df.loc[:, ['mohamed-salah',\n                                                      'roberto-firmino',\n                                                      'sadio-mane',\n                                                      'jordan-henderson',\n                                                      'fabinho',\n                                                      'georginio-wijnaldum',\n                                                      'andrew-robertson',\n                                                      'virgil-van-dijk',\n                                                      'joel-matip',\n                                                      'trent-alexander-arnold',\n                                                      'alisson']].T\n\nmatplotlib.rcParams['font.size'] = '20'\nplt.rcParams[\"axes.titlesize\"] =\"10\" \ncolor_scheme = ['#b2182b','#ef8a62','#fddbc7','#d1e5f0','#67a9cf','#2166ac']\nalt_color_scheme = ['#762a83','#af8dc3','#e7d4e8','#d9f0d3','#7fbf7b','#1b7837']\n\navailability_levels_map = {\n                        'Not at Liverpool' : -1,\n                        'Injured':0, \n                        'Not in squad':1,\n                        'Bench':2, \n                        'Played (sub)':3,\n                        'Played (starter)':4 \n                        }\nfig,(ax1,ax2) = plt.subplots(figsize=(30,10), nrows=2, sharex=True, height_ratios=[1, 4], constrained_layout=True)\n\nbar_plot = ax1.bar(cl_19_lineup_at_lpool_avail_df.index, cl_19_lineup_at_lpool_avail_df[\"strangers_count\"], align=\"edge\")\n\n# highlight champions lg final game\nfor i,bar in enumerate(bar_plot):\n    if cl_19_lineup_at_lpool_avail_df[\"strangers_count\"][i] ==11:\n        bar.set_color(\"r\")\n\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['left'].set_visible(False)\n\n# add horizontal line at 11 players\nax1.axhline(y=11, color='r', ls='--')\n\n_ = ax1.set_title(\"When stars align: The 2019 CL Final Starting 11\", fontdict={\"fontsize\":\"25\"} )\n_ = ax1.set_xlabel(\"\")\n_ = ax1.set_ylabel(\"#players\",rotation=\"vertical\", ha=\"center\")\n_ = ax1.yaxis.set_major_locator(matplotlib.ticker.FixedLocator([11]))\n_ = ax1.tick_params(length=0)\n\nnum_levels = len(availability_levels_map.keys())\nplot = sns.heatmap(heatmap_data, ax=ax2, cmap=color_scheme,linewidths=0.5, linecolor='lightgray')\ncolorbar = plot.collections[0].colorbar\ntotal_bar_height = colorbar.vmax - colorbar.vmin \nlvl_bar_height = total_bar_height/num_levels\n\n\ncolorbar.set_ticks([colorbar.vmin + (lvl_bar_height * (0.5 + i)) for i in range(num_levels)])\ncolorbar.set_ticklabels([*availability_levels_map.keys()])\n\naxs_fmtd = [cl_19_lineup_at_lpool_avail_df.loc[int(i.get_text()), 'date'].strftime(\"%d %b\") for i in ax2.get_xticklabels()]\n_ = ax2.set_xticklabels(axs_fmtd, rotation=\"vertical\", ha=\"left\")\n_ = ax2.set_xlabel(\"\")\n_ = ax2.tick_params(length=0)\n_ = plt.gcf().text(0.9, 0, \"ismailmo.com\", fontsize=25, fontdict={'color':  'blue'} )\n\n\n\n\n5 Final Visualisation\nThe final visualisation is complete, but to add some extra context I added annotations using Excalidraw to highlight some key points (probably could’ve done this in code but I got lazy):\n\n\n\n\nAnnotated heatmap\n\n\n\n\n\n6 Feedback\nAfter posting this on reddit there were some interesting suggestions for further analysis:\n\nHow does this compare with the current season?\n\n\n\n\n\n2022-23 season\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nAny thoughts, comments or ideas? Please let me know here!"
  },
  {
    "objectID": "posts/async-streamlit/async_scraping.html",
    "href": "posts/async-streamlit/async_scraping.html",
    "title": "Async web scraping and Streamlit",
    "section": "",
    "text": "Earlier this year, I built myfitnesspal wrapped: inspired by Spotify’s famous wrapped campaign, this web app scrapes all your data from myfitnesspal (a food tracking app), processes and analyses the data, and finally gives some cool (in my opinion) statistics and charts about your dietary habits in a Streamlit app.\n\n\n\nmyfitnesspal wrapped analysis\n\n\nWhile I was fairly happy about how it turned out, it annoyed me how slow it got with larger date ranges, so this article will explore how I improved this by learning about and using python’s asyncio package.\n\n1 Scraping the data\nWhile myfitnesspal does have an API - it requires you to fill out and application form and unfortunately I was unsuccessful in my application. Luckily they do not rate limit requests to the website, so although it wouldn’t be as friendly as a well-formatted JSON response, there was still a way to get all the data.\nOn observation of the network requests in the browser, the food diary page is rendered server side, so my last hope of mimicking any API calls was dead - but atleast the url was easy to reason about: just a request to the diary/{user} endpoint with a query param of the date of the food diary.\n\n\n\nsnooping around the myfitnesspal network tab\n\n\n\n\n2 The simple solution\nIn the spirit of actually finishing projects I started with a simple (naive) approach of looping through all the dates within the date range and making a request for the corresponding date on each iteration.\nFor this demo, we’ll scrape a weeks worth of data between 2022-12-15 and 2022-12-22:\n\nimport pandas as pd\nimport time\nimport requests\n\ndates = pd.date_range(\"2022-12-15\", \"2022-12-22\")\n\nLet’s define a function that makes a request for a given user and date with a request client:\n\ndef scrape_diary(user, date, client):\n    url = f\"https://www.myfitnesspal.com/food/diary/{user}?date={date}\"\n    res = client.get(url)\n    return res.text\n\nAnd then scrape the diaries by looping over each date:\n\nstart_time = time.perf_counter()\nsesh = requests.Session()\n\ndiaries =[]\n\nfor date in dates:\n    diaries.append(scrape_diary(\"ismailmo\", date, sesh))\n\n# grab total calories so we can compare with async example later\nkcals = []\nfor diary in diaries:\n    kcals.append(pd.read_html(diary, flavor=\"lxml\")[0].iloc[-4,1])\n\nelapsed = time.perf_counter() - start_time\n\nprint(f\"Time to scrape data: {elapsed:.2f} seconds\")\n\nTime to scrape data: 4.34 seconds\n\n\nThis is pretty slow with just a weeks worth of data! Given that this app is supposed to be inspired by Spotify Wrapped - we would expect users to scrape a whole years worth of food diaries. The time to scrape will scale linearly with the number of diaries, so the time to scrape a years worth of data will be ~52x longer than above! And that’s assuming our app doesn’t timeout on long request/response cycles. (spoiler alert - it does and it did)\n\n\n3 Speeding up with httpx and async\nIt seems pretty inefficient to only send one request at a time and just wait around until we get a response before sending another request - and that’s where using async python shines. It doesn’t speed up your code magically, but in scenarios like this where we are I/O bound and waiting for a response it makes a dramatic difference to the performance.\nWe’ll need to use a http client that has an async API so we import an async client from httpx to make our requests:\n\nfrom httpx import AsyncClient\nimport asyncio \nasync_client = AsyncClient()\n\n\nasync def async_scrape_diary(user, date, client):\n    url = f\"https://www.myfitnesspal.com/food/diary/{user}?date={date}\"\n    res = await client.get(url)\n    return date, res.text\n\nThere are few changes we’ve made to the previous code. Firstly we need to define the function with async def so we can use await, this returns control back to the event loop so we can start making our other requests while we wait for the response.\n\nstart_time = time.perf_counter()\nuser = \"ismailmo\"\nscraping_coroutines = []\n\nfor date in dates:\n    scraping_coroutines.append(async_scrape_diary(\"ismailmo\", date, async_client))\n\nasync_diaries = await asyncio.gather(*scraping_coroutines)\n\n# for comparison with non-async version above\nasync_kcals = []\nfor date, diary in async_diaries:\n    async_kcals.append(pd.read_html(diary, flavor=\"lxml\")[0].iloc[-4,1])\n\nasync_elapsed = time.perf_counter() - start_time\n\nprint(f\"Time to scrape data with async: {async_elapsed:.2f} seconds\")\n\nTime to scrape data with async: 0.84 seconds\n\n\nOn initial glance it may seem as though we are doing the same as above: looping over each date and scraping the diary, however since we are using the async function we do not wait for the response before continuing execution of the next iteration in the loop. You can see this in ln6 where we receive a coroutine as a return value which we add to the list of scraping_coroutines. We can then wait for all of these requests to finish by using asyncio.gather and pass it the list of coroutines (one for each diary date).\nLets do a quick sense check to make sure we got the same data back:\n\nasync_kcals == kcals\n\nTrue\n\n\nThe percentage increase in speed between the async and non async method:\n\nprint(f\"Speed up of {((elapsed - async_elapsed)/ elapsed) * 100:.2f}%\")\n\nSpeed up of 80.62%\n\n\nFor just one week’s worth of data we get a dramatic speedup but it becomes more significant as the size of the date range is greater (more pages scraped and more requests made).\n\n\n\n                                                \n\n\n\n\n4 Integrating with Streamlit\nLearning and applying async was fun in itself but ultimate goal for this optimisation is a better user experience, so the final steps are to incorporate this change into our app. Thanks to Streamlit, this is actually pretty easy. We just have to refactor our scraping function, using the principles above, and then run our app in an async loop, you can see all the changes made to go from sync -> async in this pull request.\n\n# main.py (entry point for streamlit app)\n\nasync def main():\n    # put your streamlit app setup here\n    # e.g set\n    ...\n    st.set_page_config(\n        ...\n    )\n\n    # add in other UI elements here e.g. title, input data etc\n\n    diary_df = await get_diary_for_range(start_date, end_date, mfp_user)\n \nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nAnd that’s it! Since Streamlit just runs the main.py file from top to bottom on each render, this keeps things pretty simple and we can just use asyncio like you would in any other python application or script.\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nAny thoughts, comments or ideas? Please let me know here!"
  }
]