[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi I’m Ismail, thanks for checking out my blog. I’m currently working as software engineer at Amey and I started writing just so I could get better at writing but if you’ve stumbled across this somehow, it’d be great to hear from you!\nYou can reach out to me through any of the links below:\n\n\n\n\n\n\n  \n\n\n\n\nAsync web scraping and Streamlit\n\n\n\n\nSpeeding up web scraping in a Streamlit analytics app\n\n\n\n\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Blog\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPytest and SQL\n\n\n\n\nAutomating the testing of your SQL queries\n\n\n\n\n\n\nSep 11, 2022\n\n\n\n\n\n\n  \n\n\n\n\nThe Mystery of 2019 Champions League Final\n\n\n\n\nExploring a strange statistic about the Liverpool starting 11 in the Champions League final\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\nNo matching items\n\nAny thoughts, comments or ideas? Please let me know here!"
  },
  {
    "objectID": "posts/CL19-strangers/avail-heatmaps.html",
    "href": "posts/CL19-strangers/avail-heatmaps.html",
    "title": "The Mystery of 2019 Champions League Final",
    "section": "",
    "text": "This blog was inspired by this reddit post.\nYou can skip to the final visualisation here.\nOn the night of June 1st in 2019, Liverpool would go on to win their 6th Champions League, their first since the dramatic final in 2005 and also the first major trophy won under Jurgen Klopp who joined the club in 2015. That night was significant for many reasons but a strange statistic caught my attention:\n\nThe 11 players that started in the Champions league final had never started a game together before, and has never started again since.\n\nThe rest of this article will explore the data around this statistic and uncover why this team has only ever started together once.\n\n\n\nStarting 11 for the final\n\n\nThis starting lineup is remembered by many fans as the defining team of that era and many of the comments from the reddit post capture why the statistic above is so unintuitive.\n\n\n\nComments on the stat from reddit\n\n\nSo how can this be possible? To try and uncover the mystery we can start by looking at the data from the games played during that season and extract the starting 11 from each game.\n\n1 Data Collection\nThere are many free resources for football data, but transfermarket seemed to be the best option to minimise the post-processing needed for this analysis. The code used to scrape this data can be found here - the key libraries used were pandas, beautifulsoup and requests.\n\n\nData collection and processing libraries\nimport sys\n\n# filter pandas warnings before importing\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pandas as pd\n\n# haven't published library to pypi yet so add local folder to path\n# see https://github.com/ismailmo1/football-data-viz/tree/main/transfermarket\n\nmodule_path = '/home/ismail/projects/football-data-viz/transfermarket/'\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nfrom transfermarket import leagues, players, teams\n\n\n\nprem_clubs_18 = leagues.get_prem_club_list(season=\"2018\")\nlpool_19_players = teams.get_players(team_url=prem_clubs_18['Liverpool FC'])\nlpool_19_players.keys()\n\ndict_keys(['Virgil van Dijk', 'Adam Lallana', 'Sheyi Ojo', 'Andrew Robertson', 'Alisson', 'Georginio Wijnaldum', 'Roberto Firmino', 'Dominic Solanke', 'Rhian Brewster', 'Ádám Bogdán', 'Loris Karius', 'Harry Wilson', 'Alex Oxlade-Chamberlain', 'Kamil Grabara', 'Jordan Henderson', 'Daniel Sturridge', 'Divock Origi', 'Nathaniel Clyne', 'Ben Woodburn', 'Danny Ings', 'Sadio Mané', 'Joel Matip', 'Isaac Christie-Davies', 'James Milner', 'Caoimhín Kelleher', 'Joe Gomez', 'Simon Mignolet', 'Fabinho', 'Marko Grujić', 'Ki-Jana Hoever', 'Trent Alexander-Arnold', 'Rafael Camacho', 'Xherdan Shaqiri', 'Dejan Lovren', 'Curtis Jones', 'Alberto Moreno', 'Connor Randall', 'Naby Keïta', 'Mohamed Salah'])\n\n\nWe’re only interested in the starting 11 from the final so the we can filter out the other players from the squad:\n\ncl_19_lineup = ['Trent Alexander-Arnold',\n                'Georginio Wijnaldum',\n                'Fabinho',\n                'Jordan Henderson',\n                'Roberto Firmino',\n                'Mohamed Salah',\n                'Joel Matip',\n                'Virgil van Dijk',\n                'Sadio Mané',\n                'Andrew Robertson',\n                'Alisson']\n\ncl_19_lineup_urls = {player:\"https://www.transfermarkt.com\" + url for player,\n                        url in lpool_19_players.items() if player in cl_19_lineup}\n\nFor each of these players we can grab their availability for each game.\nget_player_availability scrapes the statistics page for each player and determines which availability category a player falls into for that game.\nThe categories are:\n\nInjured\nNot in squad\nBench\nPlayed (sub)\nPlayed (starter)\n\n\navail_18_19 = []\n\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2018\")\n    avail_18_19.append(avail_df)\n\n\n\n2 Data Processing\nMost of the data processing is done in a separate module (see code here) but for this specific analysis we want to ensure we are only looking at data related to Liverpool fixtures, since the player availability data will also include international games for their country.\n\nstrangers= pd.concat(avail_18_19)\nliverpool_fix = teams.get_team_fixtures(prem_clubs_18[\"Liverpool FC\"], \"2018\")\nfixtures_num_starters = strangers.T.join(liverpool_fix, how=\"right\")\nheatmap_data = fixtures_num_starters.iloc[:, :11].T\n\nThere are also cases where the availability doesn’t fall into any of the categories above, so in that case we assume that they weren’t in the squad. This usually occurs when the player is suspended since the web-scraping logic doesn’t account for that (yet) - but there could also be other cases where transfermarket’s description doesn’t meet any of the logic defined in the web-scraping functions. We’ll see some more examples later where there are rooms for improvement in the web-scraper and the visualisations help identify these.\n\n# assumption: NaN value means they were likely suspended\n# either way they werent in the squad so makes sense \n# to assign that (to me atleast!) \nfixtures_num_starters.fillna(1, inplace=True)\nfixtures_num_starters = fixtures_num_starters.iloc[:,:11]\n\nThe number of players in the starting lineup can be calculated from the availability category they were put into: we can take a sum of all the instances of “Played (starter)” for each game.\n\n# add count\nfixtures_num_starters['strangers_count'] = (fixtures_num_starters == 4 ).sum(axis=1)\nfixtures_num_starters.reset_index(inplace=True, drop=True)\n\nThis is what our processed data looks like (showing first 5 rows only)\n\n\n\n\n\n\n  \n    \n      \n      virgil-van-dijk\n      andrew-robertson\n      alisson\n      georginio-wijnaldum\n      roberto-firmino\n      jordan-henderson\n      sadio-mane\n      joel-matip\n      fabinho\n      trent-alexander-arnold\n      mohamed-salah\n      strangers_count\n    \n  \n  \n    \n      0\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      4.0\n      1.0\n      2.0\n      4.0\n      4.0\n      8\n    \n    \n      1\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      4.0\n      2.0\n      1.0\n      4.0\n      4.0\n      8\n    \n    \n      2\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      4.0\n      3.0\n      1.0\n      4.0\n      4.0\n      8\n    \n    \n      3\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      1.0\n      4.0\n      4.0\n      9\n    \n    \n      4\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      4.0\n      3.0\n      2.0\n      4.0\n      4.0\n      8\n    \n  \n\n\n\n\n\n\n3 Initial Visualisation\nNow for the fun part! Since our data is 2 dimensional : players and dates, a heatmap seems like it would be a good way to to visualise the availability of each individual player over the course of the season, and a bar plot to keep count of how many of them started the game:\n\n\nVisualisation libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nmatplotlib.rcParams['font.family'] = 'JetBrains Mono'\nmatplotlib.rcParams['font.size'] = '20'\n\n\nWhile this plot still needs a lot of work and cleanup - it’s a nice way to visualise the data - we can see that the final column in the heatmap is the only time when all boxes are green: of course this is the night of that champions league final.\nThere are quite a few things to fix in the visualisation here:\n\nAdd titles and labels to axes\nChange y tick labels in player count bar plot to include max (11)\nReplace color bar with text instead of numbers to map availability level categories\nImprove color scheme to make it more accessible\nAlign the bars in the bar plot so it is aligned with the heatmap squares - currently it is offset\n\n\nfig,(ax1,ax2) = plt.subplots(figsize=(30,10), nrows=2, sharex=True, height_ratios=[1, 4], constrained_layout=True)\n\nbar_plot = ax1.bar(fixtures_num_starters.index, fixtures_num_starters[\"strangers_count\"])\nplot = sns.heatmap(fixtures_num_starters.iloc[:, :-1].T, ax=ax2, cmap=sns.color_palette(\"RdYlGn\"),linewidths=0.5, linecolor='lightgray')\n\n\n\n\nBefore we address the visualisation adjustments, lets grab some data for the previous season (2017/2018) so we can add some more context around the statistic:\n\n\n4 More Data Processing\n\nliverpool_fix_17_18 = teams.get_team_fixtures(prem_clubs_18[\"Liverpool FC\"], \"2017\")\n\n\navail_17_18 = []\n\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2017\", add_match_result=True)\n    # add liverpool fixture data\n    avail_df = avail_df.T.join(liverpool_fix_17_18, how=\"right\", rsuffix=\"_lpool\")\n    avail_17_18.append(avail_df.T)\n\nMore data, more problems - we now need to account for when players didn’t play for Liverpool - e.g. Virgil Van Dijk only join in January 2018 and was playing for Southampton during the first half of the season:\n\nvvd_idx = [idx for idx, df in enumerate(avail_17_18) if 'virgil' in df.index[0]][0]\n\n\navail_17_18[vvd_idx].iloc[:5, :5].dropna(axis=1)\n\n\n\n\n\n  \n    \n      Date\n      2017-08-12\n      2017-08-19\n      2017-08-23\n    \n  \n  \n    \n      virgil-van-dijk\n      0\n      0\n      0\n    \n    \n      Home team.1\n      Southampton (11.)\n      Southampton (11.)\n      Southampton\n    \n    \n      Away team.1\n      Swansea (12.)\n      West Ham (20.)\n      Wolves\n    \n    \n      Result\n      0:0\n      3:2\n      0:2\n    \n    \n      Matchday\n      1\n      2\n      Qualifying Round 2nd leg\n    \n  \n\n\n\n\nWe can account for this by adding another category to our availability levels:\n\ndef is_liverpool(rw):\n    try:\n        if 'Liverpool' in rw['Home team.1'] or 'Liverpool' in rw['Away team.1']:\n            return True\n        else:\n            return False\n    except TypeError:\n        return False \nvvd = avail_17_18[vvd_idx].T\nvvd['is_lpool'] = vvd.apply(is_liverpool, axis=1)\n\ndef add_diff_team_availability(rw):\n    \"\"\"add -1 if player isn't playing a liverpool game\"\"\"\n    if rw['is_lpool'] == False:\n        return -1\n    else:\n        # availability col\n        return rw[0]\n\nvvd.iloc[:,0] = vvd.apply(add_diff_team_availability, axis=1)\n\nThis heuristic doesn’t work when they play against Liverpool for their old club - but we can just fix that manually since it’ll be easy to spot (one game for Liverpool surrounded by loads of games for Southampton will stick out like a sore thumb)\n\nvvd.sample(5).iloc[:, :3].dropna()\n\n\n\n\n\n  \n    \n      \n      virgil-van-dijk\n      Home team.1\n      Away team.1\n    \n    \n      Date\n      \n      \n      \n    \n  \n  \n    \n      2018-05-06\n      4\n      Chelsea (5.)\n      Liverpool (4.)\n    \n    \n      2018-02-11\n      4\n      Southampton (15.)\n      Liverpool (3.)\n    \n    \n      2018-04-21\n      4\n      West Brom (20.)\n      Liverpool (3.)\n    \n    \n      2017-11-29\n      -1\n      Man City (1.)\n      Southampton (10.)\n    \n    \n      2018-02-04\n      4\n      Liverpool (3.)\n      Spurs (5.)\n    \n  \n\n\n\n\n\n\nLet’s now grab data for the seasons before and after (17/18 and 19/20, 20/21 and 21/22)\n# should have refactored the data collection/processing loop into a function\n\navail_17_18 = []\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2017\", add_match_result=True)\n    # add liverpool fixture data\n    transposed_df = avail_df.T\n    transposed_df= transposed_df.join(liverpool_fix_17_18, how=\"right\", rsuffix=\"_lpool\")\n\n    transposed_df['is_lpool'] = transposed_df.apply(is_liverpool, axis=1)\n    transposed_df.iloc[:,0] = transposed_df.apply(add_diff_team_availability, axis=1)\n    transposed_df = pd.DataFrame(transposed_df.iloc[:,0]).T\n    avail_17_18.append(transposed_df)\n    \navail_17_18_all = pd.concat(avail_17_18)\navail_17_18_all.fillna(1, inplace=True)\n\n# add count\nlong_avail_17_18_df = avail_17_18_all.T\nlong_avail_17_18_df['strangers_count'] = (long_avail_17_18_df == 4 ).sum(axis=1)\nlong_avail_17_18_df.reset_index(inplace=True, drop=True)\n#| echo: false\nliverpool_fix_19_20 = teams.get_team_fixtures(prem_clubs_18[\"Liverpool FC\"], \"2019\")\navail_19_20 = []\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2019\", add_match_result=True)\n    # add liverpool fixture data\n    transposed_df = avail_df.T\n    transposed_df= transposed_df.join(liverpool_fix_19_20, how=\"right\", rsuffix=\"_lpool\")\n\n    transposed_df['is_lpool'] = transposed_df.apply(is_liverpool, axis=1)\n    transposed_df.iloc[:,0] = transposed_df.apply(add_diff_team_availability, axis=1)\n    transposed_df = pd.DataFrame(transposed_df.iloc[:,0]).T\n    avail_19_20.append(transposed_df)\n    \navail_19_20_all = pd.concat(avail_19_20)\navail_19_20_all.fillna(1, inplace=True)\n\n# add count\nlong_avail_19_20_df = avail_19_20_all.T\nlong_avail_19_20_df['strangers_count'] = (long_avail_19_20_df == 4 ).sum(axis=1)\nlong_avail_19_20_df.reset_index(inplace=True, drop=True)\n\n#add 2020/2021\n\nliverpool_fix_20_21 = teams.get_team_fixtures(prem_clubs_18[\"Liverpool FC\"], \"2020\")\navail_20_21 = []\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2020\", add_match_result=True)\n    # add liverpool fixture data\n    transposed_df = avail_df.T\n    transposed_df= transposed_df.join(liverpool_fix_20_21, how=\"right\", rsuffix=\"_lpool\")\n\n    transposed_df['is_lpool'] = transposed_df.apply(is_liverpool, axis=1)\n    transposed_df.iloc[:,0] = transposed_df.apply(add_diff_team_availability, axis=1)\n    transposed_df = pd.DataFrame(transposed_df.iloc[:,0]).T\n    avail_20_21.append(transposed_df)\n    \navail_20_21_all = pd.concat(avail_20_21)\navail_20_21_all.fillna(1, inplace=True)\n\n# add count\nlong_avail_20_21_df = avail_20_21_all.T\nlong_avail_20_21_df['strangers_count'] = (long_avail_20_21_df == 4 ).sum(axis=1)\nlong_avail_20_21_df.reset_index(inplace=True, drop=True)\n#add 2021/2022\n\nliverpool_fix_21_22 = teams.get_team_fixtures(prem_clubs_18[\"Liverpool FC\"], \"2021\")\navail_21_22 = []\nfor player,url in cl_19_lineup_urls.items():\n    avail_df = players.get_player_availability(url,\"2021\", add_match_result=True)\n    # add liverpool fixture data\n    transposed_df = avail_df.T\n    transposed_df= transposed_df.join(liverpool_fix_21_22, how=\"right\", rsuffix=\"_lpool\")\n\n    transposed_df['is_lpool'] = transposed_df.apply(is_liverpool, axis=1)\n    transposed_df.iloc[:,0] = transposed_df.apply(add_diff_team_availability, axis=1)\n    transposed_df = pd.DataFrame(transposed_df.iloc[:,0]).T\n    avail_21_22.append(transposed_df)\n    \navail_21_22_all = pd.concat(avail_21_22)\navail_21_22_all.fillna(1, inplace=True)\n\n# add count\nlong_avail_21_22_df = avail_21_22_all.T\nlong_avail_21_22_df['strangers_count'] = (long_avail_21_22_df == 4 ).sum(axis=1)\nlong_avail_21_22_df.reset_index(inplace=True, drop=True)\n\n# concatenate all our data \nall_fix_dates = [*avail_17_18_all.columns, *liverpool_fix.index,*avail_19_20_all.columns, *avail_20_21_all.columns,*avail_21_22_all.columns]\nall_avail = pd.concat([long_avail_17_18_df, fixtures_num_starters, long_avail_19_20_df, long_avail_20_21_df,long_avail_21_22_df])\n\n# reset index so we dont have to deal with dodgy dates in axes\nall_avail.reset_index(inplace=True, drop=True)\nall_avail['date'] = all_fix_dates\nall_avail.tail()\n\n\n\n\n\n\n  \n    \n      \n      virgil-van-dijk\n      andrew-robertson\n      alisson\n      georginio-wijnaldum\n      roberto-firmino\n      jordan-henderson\n      sadio-mane\n      joel-matip\n      fabinho\n      trent-alexander-arnold\n      mohamed-salah\n      strangers_count\n      date\n    \n  \n  \n    \n      282\n      4.0\n      1.0\n      4.0\n      -1.0\n      2.0\n      3.0\n      4.0\n      4.0\n      4.0\n      4.0\n      3.0\n      6\n      2022-05-10\n    \n    \n      283\n      4.0\n      4.0\n      4.0\n      -1.0\n      3.0\n      4.0\n      4.0\n      3.0\n      0.0\n      4.0\n      4.0\n      7\n      2022-05-14\n    \n    \n      284\n      0.0\n      2.0\n      4.0\n      -1.0\n      4.0\n      3.0\n      1.0\n      4.0\n      0.0\n      1.0\n      1.0\n      3\n      2022-05-17\n    \n    \n      285\n      2.0\n      4.0\n      4.0\n      -1.0\n      3.0\n      4.0\n      4.0\n      4.0\n      0.0\n      4.0\n      3.0\n      6\n      2022-05-22\n    \n    \n      286\n      4.0\n      4.0\n      4.0\n      -1.0\n      3.0\n      4.0\n      4.0\n      2.0\n      4.0\n      4.0\n      4.0\n      8\n      2022-05-28\n    \n  \n\n\n\n\nTo address some of the ugliness from the initial visualisation we can make some changes in the visualisation setup (expand code and see comments):\n\n\nVisualisation setup\navailability_levels_map = {\n                        'Not at Liverpool' : -1,\n                        'Injured':0, \n                        'Not in squad':1,\n                        'Bench':2, \n                        'Played (sub)':3,\n                        'Played (starter)':4 \n                        }\nfig,(ax1,ax2) = plt.subplots(figsize=(30,10), nrows=2, sharex=True, height_ratios=[1, 4], constrained_layout=True)\n\n# align bars to edge so its aligned with the heatmap\nbar_plot = ax1.bar(all_avail.index, all_avail[\"strangers_count\"], align=\"edge\")\n#  remove x axis label (since we share with heatmap)\n_ = ax1.set_xlabel(\"\")\n_ = ax1.tick_params(length=0)\n# add bar graph y label and only add tick label to max #players\n_ = ax1.set_ylabel(\"#players starting\",rotation=\"horizontal\", ha=\"right\" )\n_ = ax1.yaxis.set_major_locator(matplotlib.ticker.FixedLocator([11]))\n_ = ax1.set_title(\"When stars align: The Mythical 2019 CL Final Starting 11\")\n\nnum_levels = len(availability_levels_map.keys())\nplot = sns.heatmap(all_avail.iloc[:, :-2].T, ax=ax2, cmap=sns.color_palette(\"RdYlGn\",num_levels),linewidths=0.5, linecolor='lightgray')\n\n# map colorbar labels with availability levels\ncolorbar = plot.collections[0].colorbar\ntotal_bar_height = colorbar.vmax - colorbar.vmin \nlvl_bar_height = total_bar_height/num_levels\n\ncolorbar.set_ticks([colorbar.vmin + (lvl_bar_height * (0.5 + i)) for i in range(num_levels)])\ncolorbar.set_ticklabels([*availability_levels_map.keys()])\n\n# gotta love dates - format them so they look cleaner\naxs_fmtd = [all_avail.loc[int(i.get_text()), 'date'].strftime(\"%d %b\") for i in ax2.get_xticklabels()]\n_ = ax2.set_xticklabels(axs_fmtd, rotation=\"vertical\")\n_ = ax2.set_xlabel(\"\")\n_ = ax2.tick_params(length=0)\n_ = plt.gcf().text(0.9, 0, \"ismailmo.com\", fontsize=12, fontdict={'color':  'blue'} )\n\n\n\n\n\nThere are some anomalies where players are marked as “Not at liverpool” even though they definitely were (see Trent, Mane and salah’s red lines)\nMane and Salah were playing eachother at the AFCON final - so technically the graph is correct :)\n\n\n\n\n\n\n  \n    \n      \n      sadio-mane\n      date\n    \n  \n  \n    \n      258\n      -1.0\n      2022-02-06\n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      sadio-mane\n      date\n    \n  \n  \n    \n      258\n      -1.0\n      2022-02-06\n    \n  \n\n\n\n\nFor Trent, the England U21 team were playing at the same time (which Trent used to play for) so our logic that adjusted for games played for other clubs (e.g. Van Dijk for Southampton) picked up this game- since this is a rare edge case I’ll just overwrite it manually\n\n\n\n\n\n\n  \n    \n      Date\n      2018-05-26\n      2018-05-26\n    \n  \n  \n    \n      trent-alexander-arnold\n      4\n      1\n    \n    \n      Home team.1\n      Real Madrid\n      England U21\n    \n    \n      Away team.1\n      Liverpool\n      China U20\n    \n    \n      Result\n      3:1\n      2:1\n    \n  \n\n\n\n\n\n\nManually adjusting outlier for Trent\ntrent_outlier = all_avail['date']==datetime.datetime(2018,5,26)\nall_avail.loc[trent_outlier,\"trent-alexander-arnold\"] = 4\n\n\nThe visualisation above shows that the full squad were only playing for Liverpool FC during 2018/19 - 2021-22 (see the blocks of red at the start and end of the heatmap) - we can remove the other seasons (we’ll keep a few rows to highlight their departure/arrival at Liverpool).\nAnother key change is choosing a better colours palette - ColorBrewer is a great resource for finding accessible color schemes based on the research of Dr Cynthia Brewer\n\n\nNarrowing down the date range\ncl_19_lineup_at_lpool_avail_df = all_avail[(all_avail.loc[:,'date']>datetime.datetime(2018,5,10)) &( all_avail.loc[:,'date']<datetime.datetime(2021,8,30))]\ncl_19_lineup_at_lpool_avail_df.reset_index(inplace=True, drop=True)\n\n\n\n\nFinal visualisation setup\n# order players by position\nheatmap_data = cl_19_lineup_at_lpool_avail_df.loc[:, ['mohamed-salah',\n                                                      'roberto-firmino',\n                                                      'sadio-mane',\n                                                      'jordan-henderson',\n                                                      'fabinho',\n                                                      'georginio-wijnaldum',\n                                                      'andrew-robertson',\n                                                      'virgil-van-dijk',\n                                                      'joel-matip',\n                                                      'trent-alexander-arnold',\n                                                      'alisson']].T\n\nmatplotlib.rcParams['font.size'] = '20'\nplt.rcParams[\"axes.titlesize\"] =\"10\" \ncolor_scheme = ['#b2182b','#ef8a62','#fddbc7','#d1e5f0','#67a9cf','#2166ac']\nalt_color_scheme = ['#762a83','#af8dc3','#e7d4e8','#d9f0d3','#7fbf7b','#1b7837']\n\navailability_levels_map = {\n                        'Not at Liverpool' : -1,\n                        'Injured':0, \n                        'Not in squad':1,\n                        'Bench':2, \n                        'Played (sub)':3,\n                        'Played (starter)':4 \n                        }\nfig,(ax1,ax2) = plt.subplots(figsize=(30,10), nrows=2, sharex=True, height_ratios=[1, 4], constrained_layout=True)\n\nbar_plot = ax1.bar(cl_19_lineup_at_lpool_avail_df.index, cl_19_lineup_at_lpool_avail_df[\"strangers_count\"], align=\"edge\")\n\n# highlight champions lg final game\nfor i,bar in enumerate(bar_plot):\n    if cl_19_lineup_at_lpool_avail_df[\"strangers_count\"][i] ==11:\n        bar.set_color(\"r\")\n\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['left'].set_visible(False)\n\n# add horizontal line at 11 players\nax1.axhline(y=11, color='r', ls='--')\n\n_ = ax1.set_title(\"When stars align: The 2019 CL Final Starting 11\", fontdict={\"fontsize\":\"25\"} )\n_ = ax1.set_xlabel(\"\")\n_ = ax1.set_ylabel(\"#players\",rotation=\"vertical\", ha=\"center\")\n_ = ax1.yaxis.set_major_locator(matplotlib.ticker.FixedLocator([11]))\n_ = ax1.tick_params(length=0)\n\nnum_levels = len(availability_levels_map.keys())\nplot = sns.heatmap(heatmap_data, ax=ax2, cmap=color_scheme,linewidths=0.5, linecolor='lightgray')\ncolorbar = plot.collections[0].colorbar\ntotal_bar_height = colorbar.vmax - colorbar.vmin \nlvl_bar_height = total_bar_height/num_levels\n\n\ncolorbar.set_ticks([colorbar.vmin + (lvl_bar_height * (0.5 + i)) for i in range(num_levels)])\ncolorbar.set_ticklabels([*availability_levels_map.keys()])\n\naxs_fmtd = [cl_19_lineup_at_lpool_avail_df.loc[int(i.get_text()), 'date'].strftime(\"%d %b\") for i in ax2.get_xticklabels()]\n_ = ax2.set_xticklabels(axs_fmtd, rotation=\"vertical\", ha=\"left\")\n_ = ax2.set_xlabel(\"\")\n_ = ax2.tick_params(length=0)\n_ = plt.gcf().text(0.9, 0, \"ismailmo.com\", fontsize=25, fontdict={'color':  'blue'} )\n\n\n\n\n5 Final Visualisation\nThe final visualisation is complete, but to add some extra context I added annotations using Excalidraw to highlight some key points (probably could’ve done this in code but I got lazy):\n\n\n\n\nAnnotated heatmap\n\n\n\n\n\n6 Feedback\nAfter posting this on reddit there were some interesting suggestions for further analysis:\n\nHow does this compare with the current season?\n\n\n\n\n\n2022-23 season\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nAny thoughts, comments or ideas? Please let me know here!"
  },
  {
    "objectID": "posts/async-streamlit/async_scraping.html",
    "href": "posts/async-streamlit/async_scraping.html",
    "title": "Async web scraping and Streamlit",
    "section": "",
    "text": "Earlier this year, I built myfitnesspal wrapped: inspired by Spotify’s famous wrapped campaign, this web app scrapes all your data from myfitnesspal (a food tracking app), processes and analyses the data, and finally gives some cool (in my opinion) statistics and charts about your dietary habits.\n\n\n\nmyfitnesspal wrapped analysis\n\n\nWhile myfitnesspal does have an API - it requires you to fill out and application form and unfortunately I was unsuccessful in my application. Luckily they do not rate limit requests to the website, so although it wouldn’t be as friendly as a well-formatted JSON response, there was still a way to get all the data.\nOn observation of the network requests in the browser, the food diary page is rendered server side, so our last hope of mimicing any API calls was dead - but atleast the url was easy to reason about: just a request to the diary/{user} endpoint with a query param of the date of the food diary.\n\n\n\nsnooping around the myfitnesspal network tab\n\n\nIn the spirit of actually finishing projects I started with a simple (naive) approach of looping through all the dates within the date range and making a request to the corresponding date:\n\nimport pandas as pd\nimport sys\nimport asyncio\nimport time\n\nDefine a 2 month date range to give us enough requests to benchmark on\n\ndates = pd.date_range(\"2022-10-01\", \"2022-11-29\")\n\nTry the standard way of one request at a time\n\nimport requests\n\nstart_time = time.perf_counter()\n\nsesh = requests.Session()\n\ndef scrape_diary(user, date, client):\n    url = f\"https://www.myfitnesspal.com/food/diary/{user}?date={date}\"\n    res = client.get(url)\n    return res.text\ndiaries =[]\nfor date in dates:\n    diaries.append(scrape_diary(\"ismailmo\", date, sesh))\n\nkcals = []\nfor diary in diaries:\n    kcals.append(pd.read_html(diary, flavor=\"lxml\")[0].iloc[-4,1])\n\nelapsed = time.perf_counter() - start_time\n\nprint(f\"time to scrape data: {elapsed:.2f} seconds\")\n\n\nfrom httpx import AsyncClient\n\nstart_time = time.perf_counter()\n\nasync_client = AsyncClient()\ndate_param = \"2022-11-29\"\nuser = \"ismailmo\"\n\nasync def async_scrape_diary(user, date, client):\n    url = f\"https://www.myfitnesspal.com/food/diary/{user}?date={date}\"\n    res = await client.get(url)\n    return date, res.text\ncoros = []\nasync_diaries =[]\nfor date in dates:\n    coros.append(async_scrape_diary(\"ismailmo\", date, async_client))\n\nasync_diaries = await asyncio.gather(*coros)\n\nasync_kcals = []\nfor date, diary in async_diaries:\n    async_kcals.append(pd.read_html(diary, flavor=\"lxml\")[0].iloc[-4,1])\n\nasync_elapsed = time.perf_counter() - start_time\n\nprint(f\"time to scrape data: {async_elapsed:.2f} seconds\")\n\n\nprint(f\"Speed up of {((elapsed - async_elapsed)/ elapsed) * 100:.2f}%\")\n\nThis is a huge increase in speed and it’s more significant as the size of the date range is greater (more pages scraped and more requests made), lets make sure the data we get back is the same:\n\nasync_kcals == kcals\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nAny thoughts, comments or ideas? Please let me know here!"
  },
  {
    "objectID": "posts/pytest-sql/index.html",
    "href": "posts/pytest-sql/index.html",
    "title": "Pytest and SQL",
    "section": "",
    "text": "The source code for the demo referenced in this post can be found here. You can jump to the actual testing part here (skip all the intro stuff)\nAny thoughts, comments or ideas? Please let me know here!"
  },
  {
    "objectID": "posts/pytest-sql/index.html#instantiate-the-test-database",
    "href": "posts/pytest-sql/index.html#instantiate-the-test-database",
    "title": "Pytest and SQL",
    "section": "5.1 Instantiate the test database",
    "text": "5.1 Instantiate the test database\nBefore doing any querying or testing, we need to start a database instance - docker makes this quite painless.\nWe’ll define the image in a Dockerfile and add some seed data that we’ll need for our query. We could’ve chosen to add this data in our tests or fixtures, but since this is reference data and is not specific to a test, we can add it in during the image build.\nThe seed data script creates the table and adds the reference data:\nCREATE TABLE public.food_ref (\n    id varchar NOT NULL,\n    \"name\" varchar NULL,\n    \"kcal/unit\" int4 NULL,\n    unit_of_measure varchar NULL,\n    CONSTRAINT food_ref_pkey PRIMARY KEY (id)\n);\n\nINSERT INTO\n    public.food_ref (id, \"name\", \"kcal/unit\", unit_of_measure)\nVALUES\n('a', 'bread', 10, 'g')\n-- we can add more values here;\nThe postgres docker image will run any .sql (or .sh) scripts in docker-entrypoint-initdb.d/ so we can copy our seed script there:\nFROM postgres:10-alpine\nENV POSTGRES_USER=postgres\nENV POSTGRES_PASSWORD=password\nEXPOSE 5432\n# seed database with food reference data\nCOPY ./seed_scripts/food_ref.sql /docker-entrypoint-initdb.d/"
  },
  {
    "objectID": "posts/pytest-sql/index.html#setup-fixtures",
    "href": "posts/pytest-sql/index.html#setup-fixtures",
    "title": "Pytest and SQL",
    "section": "5.2 Setup fixtures",
    "text": "5.2 Setup fixtures\nWe can define a session-scope fixture which uses the docker library to build the image and run the container for our tests. Using yield here means that once the tests have run, control is returned back to this fixture and we can remove the container and image so we return to a clean state.\n@pytest.fixture(scope=\"session\")\ndef postgres_container():\n    \"\"\"start docker instance of postgres\"\"\"\n    client = docker.from_env()\n\n    client.images.build(\n        path=TEST_DB_DOCKERFILE_PATH, tag=\"test_postgres\", nocache=True\n    )\n\n    try:\n        # create container\n        db_container = client.containers.run(\n            \"test_postgres\",\n            name=\"test_db\",\n            detach=True,\n            ports={5432: 5432},\n        )\n\n    except APIError:\n        # docker api returns an error sincewe already have a container\n        # remove existing container and make a new one\n        db_container = client.containers.get(\"test_db\")\n        db_container.remove()  # type:ignore\n        db_container = client.containers.run(\n            \"test_postgres\",\n            name=\"test_db\",\n            detach=True,\n            ports={5432: 5432},\n        )\n    # return to calling function so we can use the container\n    yield\n    # calling function is done so we can stop and remove the container\n    db_container.stop()  # type:ignore\n    db_container.remove()  # type:ignore\nThe only other setup we need is to create a fixture for a session that runs on the test database container above, this will allow us to define some steps to connect, run our tests and then cleanup the database to remove all the data we added for our test:\n@pytest.fixture\ndef test_session(postgres_container) -> Iterator[Session]:\n    \"\"\"create db session and cleanup data after each test\"\"\"\n    # testing connection string\n    DOCKER_USERNAME = \"postgres\"\n    DOCKER_PASSWORD = \"password\"\n    DOCKER_HOSTNAME = \"localhost:5432\"\n    DOCKER_DB_NAME = \"postgres\"\n\n    sql_url = (\n        f\"postgresql://{DOCKER_USERNAME}:{DOCKER_PASSWORD}@\"\n        f\"{DOCKER_HOSTNAME}/{DOCKER_DB_NAME}\"\n    )\n    engine = create_engine(sql_url, pool_pre_ping=True)\n\n    # wait until db is ready\n    MAX_RETRIES = 100\n    #  max value for backoff time\n    MAX_RETRY_SLEEP_SEC = 2\n\n    num_retries = 0\n    try:\n        while num_retries < MAX_RETRIES:\n            num_retries += 1\n            # keep increasing back off time until MAX_RETRY_SLEEP_SEC\n            # using (truncated) exponential backoff\n            sleep_time_ms = min(\n                [MAX_RETRY_SLEEP_SEC * 1000, 100 * num_retries]\n            )\n            try:\n                # check if db is ready with simple query\n                engine.execute(\"SELECT 1\")\n            except OperationalError:\n                # db is still starting up, continue loop and retry\n                sleep(sleep_time_ms / 1000)\n                continue\n            # db has started - lets break out of loop\n            break\n    except OperationalError:\n        raise Exception(\"Couldn't connect to Test Postgres Docker Instance!\")\n\n    # create all tables registered to our declarative base class\n    Base.metadata.create_all(engine)\n    with Session(engine) as session:\n        yield session\n        session.commit()\n\n    # clean up our db\n    Base.metadata.drop_all(engine)\nThe logic in ln16 - ln38 ensures that we do not try to create our session before the database in the container is ready. The client.containers.run function in the postgres_container fixture returns before the database is ready since the container has already started successfully."
  },
  {
    "objectID": "posts/pytest-sql/index.html#the-payoff",
    "href": "posts/pytest-sql/index.html#the-payoff",
    "title": "Pytest and SQL",
    "section": "5.3 The payoff",
    "text": "5.3 The payoff\nFinally we can actually test our query:\ndef test_get_net_calories(test_session: Session):\n    food_cons = FoodConsumption(\n        food_id=\"a\", qty=\"100\", date=datetime.date.today()\n    )\n    bike_session = Bike(\n        date=datetime.date.today(), speed_mph=5, duration_min=20\n    )\n    with test_session:\n        test_session.add_all([food_cons, bike_session])\n        test_session.commit()\n\n    result = get_total_net_calories(test_session)\n\n    assert result == 980\nBreaking down what happens when we run our test:\n\nln1 we pass in test_session to our test which call the test_session fixture above\nThe test_session fixture call the postgres_container fixture\nThe db image is built, seeded with reference data and the container is started\nA session is created after connecting to the db container\nln2 - ln10 we add some data specific to our test (ARRANGE)\nln12 we run our function under test and pass it the test_session (ACT)\nln14 we ensure the data is what we expect (ASSERT)\n\nReturn to the test_session fixture and delete all data in the db (i.e. undo step 5.)\nReturn to the postgres_container fixture and stop the container and remove it.\n???\nPROFIT!!!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "Async web scraping and Streamlit\n\n\n1 min\n\n\n\npython\n\n\nstreamlit\n\n\nasyncio\n\n\nweb-scraping\n\n\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Mystery of 2019 Champions League Final\n\n\n5 min\n\n\n\nweb-scraping\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\ndata-viz\n\n\nfootball\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytest and SQL\n\n\n14 min\n\n\n\nsql\n\n\npython\n\n\npytest\n\n\n\n\nSep 11, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\nAny thoughts, comments or ideas? Please let me know here!"
  }
]