{
  "hash": "5050e7428eccb5ce3bc46e70d37eedb7",
  "result": {
    "markdown": "---\ntitle: Async web scraping and Streamlit\ndescription: Speeding up web scraping in a Streamlit analytics app\ndate: 21-12-2022\ncategories:\n  - python\n  - streamlit\n  - asyncio\n  - web-scraping\nexecute:\n  eval: false\nimage: pytest_sql.png\nformat:\n  html:\n    toc: true\n    toc-location: left\n    number-sections: true\n    code-line-numbers: true\n---\n\nEarlier this year, I built [myfitnesspal wrapped](https://wrapped.ismailmo.com): inspired by Spotify's famous wrapped campaign, this web app scrapes all your data from myfitnesspal (a food tracking app), processes and analyses the data, and finally gives some cool (in my opinion) statistics and charts about your dietary habits.\n\n![myfitnesspal wrapped analysis](https://github.com/ismailmo1/mfp-wrapped/raw/develop/app/images/card_preview.png)\n\nWhile myfitnesspal does have an API - it requires you to fill out and application form and unfortunately I was unsuccessful in my application. Luckily they do not rate limit requests to the website, so although it wouldn't be as friendly as a well-formatted JSON response, there was still a way to get all the data.\n\nOn observation of the network requests in the browser, the food diary page is rendered server side, so our last hope of mimicing any API calls was dead - but atleast the url was easy to reason about: just a request to the `diary/{user}` endpoint with a query param of the date of the food diary.\n\n![snooping around the myfitnesspal network tab](./mfp-network-tab.png)\n\nIn the spirit of actually finishing projects I started with a simple (naive) approach of looping through all the dates within the date range and making a request to the corresponding date:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport sys\nimport asyncio\nimport time\n```\n:::\n\n\nDefine a 2 month date range to give us enough requests to benchmark on \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndates = pd.date_range(\"2022-10-01\", \"2022-11-29\")\n```\n:::\n\n\nTry the standard way of one request at a time\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport requests\n\nstart_time = time.perf_counter()\n\nsesh = requests.Session()\n\ndef scrape_diary(user, date, client):\n    url = f\"https://www.myfitnesspal.com/food/diary/{user}?date={date}\"\n    res = client.get(url)\n    return res.text\ndiaries =[]\nfor date in dates:\n    diaries.append(scrape_diary(\"ismailmo\", date, sesh))\n\nkcals = []\nfor diary in diaries:\n    kcals.append(pd.read_html(diary, flavor=\"lxml\")[0].iloc[-4,1])\n\nelapsed = time.perf_counter() - start_time\n\nprint(f\"time to scrape data: {elapsed:.2f} seconds\")\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom httpx import AsyncClient\n\nstart_time = time.perf_counter()\n\nasync_client = AsyncClient()\ndate_param = \"2022-11-29\"\nuser = \"ismailmo\"\n\nasync def async_scrape_diary(user, date, client):\n    url = f\"https://www.myfitnesspal.com/food/diary/{user}?date={date}\"\n    res = await client.get(url)\n    return date, res.text\ncoros = []\nasync_diaries =[]\nfor date in dates:\n    coros.append(async_scrape_diary(\"ismailmo\", date, async_client))\n\nasync_diaries = await asyncio.gather(*coros)\n\nasync_kcals = []\nfor date, diary in async_diaries:\n    async_kcals.append(pd.read_html(diary, flavor=\"lxml\")[0].iloc[-4,1])\n\nasync_elapsed = time.perf_counter() - start_time\n\nprint(f\"time to scrape data: {async_elapsed:.2f} seconds\")\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint(f\"Speed up of {((elapsed - async_elapsed)/ elapsed) * 100:.2f}%\")\n```\n:::\n\n\nThis is a huge increase in speed and it's more significant as the size of the date range is greater (more pages scraped and more requests made), lets make sure the data we get back is the same:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nasync_kcals == kcals\n```\n:::\n\n\n",
    "supporting": [
      "async_scraping_files"
    ],
    "filters": [],
    "includes": {}
  }
}