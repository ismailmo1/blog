{
  "hash": "834e063da0950d7bb62c08d1731c0bd3",
  "result": {
    "markdown": "---\ntitle: Async web scraping and Streamlit\ndescription: Speeding up web scraping in a Streamlit analytics app\ndate: 12-22-2022\ncategories:\n  - python\n  - streamlit\n  - asyncio\n  - web-scraping\nexecute:\n  eval: true\nimage: pytest_sql.png\nformat:\n  html:\n    toc: true\n    toc-location: left\n    number-sections: true\n    code-line-numbers: true\n---\n\nEarlier this year, I built [myfitnesspal wrapped](https://wrapped.ismailmo.com): inspired by Spotify's famous wrapped campaign, this web app scrapes all your data from myfitnesspal (a food tracking app), processes and analyses the data, and finally gives some cool (in my opinion) statistics and charts about your dietary habits in a [Streamlit](https://streamlit.io) app.\n\n![myfitnesspal wrapped analysis](https://github.com/ismailmo1/mfp-wrapped/raw/develop/app/images/card_preview.png)\n\nWhile I was fairly happy about how it turned out, it annoyed me how slow it got with larger date range, so this article will explore how I improved this by using python's async features.\n\n# Scraping the data\nWhile myfitnesspal does have an API - it requires you to fill out and application form and unfortunately I was unsuccessful in my application. Luckily they do not rate limit requests to the website, so although it wouldn't be as friendly as a well-formatted JSON response, there was still a way to get all the data.\n\nOn observation of the network requests in the browser, the food diary page is rendered server side, so my last hope of mimicking any API calls was dead - but atleast the url was easy to reason about: just a request to the `diary/{user}` endpoint with a query param of the date of the food diary.\n\n![snooping around the myfitnesspal network tab](./mfp-network-tab.png)\n\n# The simple solution\n\nIn the spirit of actually finishing projects I started with a simple (naive) approach of looping through all the dates within the date range and making a request for the corresponding date on each iteration.\n\nFor this demo, we'll scrape a weeks worth of data between 2022-12-15 and 2022-12-22:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport time\nimport requests\n\ndates = pd.date_range(\"2022-12-15\", \"2022-12-22\")\n```\n:::\n\n\nLet's define a function that makes a request for a given user and date with a request client:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef scrape_diary(user, date, client):\n    url = f\"https://www.myfitnesspal.com/food/diary/{user}?date={date}\"\n    res = client.get(url)\n    return res.text\n```\n:::\n\n\nAnd then scrape the diaries by looping over each date:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nstart_time = time.perf_counter()\nsesh = requests.Session()\n\ndiaries =[]\n\nfor date in dates:\n    diaries.append(scrape_diary(\"ismailmo\", date, sesh))\n\n# grab total calories so we can compare with async example later\nkcals = []\nfor diary in diaries:\n    kcals.append(pd.read_html(diary, flavor=\"lxml\")[0].iloc[-4,1])\n\nelapsed = time.perf_counter() - start_time\n\nprint(f\"Time to scrape data: {elapsed:.2f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime to scrape data: 4.16 seconds\n```\n:::\n:::\n\n\nThis is pretty slow with just a weeks worth of data! Given that this app is supposed to be inspired by Spotify Wrapped - we would expect users to scrape a whole years worth of food diaries. The time to scrape will scale linearly with the number of diaries, so the time to scrape a years worth of data will be ~52x longer than above! \nAnd that's assuming our app doesn't timeout on long request/response cycles. ([spoiler alert - it does and it did](https://github.com/ismailmo1/mfp-wrapped/pull/7))\n\n# Speeding up with httpx and async\n\nIt seems pretty inefficient to only send one request at a time and just wait around until we get a response before sending another request - and that's where using async python shines. It doesn't speed up your code magically, but in scenarios like this where we are I/O bound and waiting for a response it makes a dramatic difference to the performance.\n\nWe'll need to use a http client that has an async API so we import an async client from `httpx` to make our requests:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom httpx import AsyncClient\nimport asyncio \nasync_client = AsyncClient()\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nasync def async_scrape_diary(user, date, client):\n    url = f\"https://www.myfitnesspal.com/food/diary/{user}?date={date}\"\n    res = await client.get(url)\n    return date, res.text\n```\n:::\n\n\nThere are few changes we've made to the previous code, firstly we need to define the function with `async def` so we can use `await` which will return control back to the event loop so we can continue making our other request while we wait for the response.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nstart_time = time.perf_counter()\nuser = \"ismailmo\"\nscraping_coroutines = []\n\nfor date in dates:\n    scraping_coroutines.append(async_scrape_diary(\"ismailmo\", date, async_client))\n\nasync_diaries = await asyncio.gather(*scraping_coroutines)\n\n# for comparison with non-async version above\nasync_kcals = []\nfor date, diary in async_diaries:\n    async_kcals.append(pd.read_html(diary, flavor=\"lxml\")[0].iloc[-4,1])\n\nasync_elapsed = time.perf_counter() - start_time\n\nprint(f\"Time to scrape data with async: {async_elapsed:.2f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime to scrape data with async: 0.82 seconds\n```\n:::\n:::\n\n\nOn initial glance it may seem as though we are doing the same as above: looping over each date and scraping the diary, however since we are using the async function we do not wait for the response before continuing execution of the next iteration in the loop. That is in `ln6` we receive a coroutine as a return which we add to the list of `scraping_coroutines`. We can then wait for all of these requests to finish by using `asyncio.gather` and pass it the list of coroutines (one for each diary date).\n\nLets do a quick sense check to make sure we got the same data back:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nasync_kcals == kcals\n```\n\n::: {.cell-output .cell-output-display execution_count=63}\n```\nTrue\n```\n:::\n:::\n\n\nThe percentage increase in speed between the async and non async method:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nprint(f\"Speed up of {((elapsed - async_elapsed)/ elapsed) * 100:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSpeed up of 80.32%\n```\n:::\n:::\n\n\nFor just one week's worth of data we get a dramatic speedup but it becomes more significant as the size of the date range is greater (more pages scraped and more requests made).\n\n",
    "supporting": [
      "async_scraping_files"
    ],
    "filters": [],
    "includes": {}
  }
}