{
  "hash": "2ed9cc0bf2122b833c0110272cedd372",
  "result": {
    "markdown": "---\ntitle: Pytest and SQL\ndate: 09-11-2022\ncategories:\n  - sql\n  - python\n  - pytest\nimage: pytest_sql.png\nformat:\n  html:\n    toc: true\n    toc-location: left\n    number-sections: true\n    code-line-numbers: true\n---\n\nThe code referenced in this post can be found [here](https://github.com/ismailmo1/pytest-sql)\n\nJump to the actual [testing part](#automated-testing) (skip all the intro stuff)\n\n# SQL in Python\nEmbedding SQL queries into your python code may not be the ideal approach in every scenario, however sometimes you're in a situation (by your own making or others') where you have a complex SQL query sitting in your python application somewhere, and you want to test it just like you test your other functions. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\ndef get_some_data(session):\n    super_critical_sql_query = \"SELECT col1, col2 FROM some_table\"\"\"\n    data_rows = session.execute(super_critical_sql_query)   \n    return data_rows\n```\n:::\n\n\nThinking about how we would test this manually:\n\n1. Connect to a dev/test instance of your db \n2. Copy/paste the sql query into a client (e.g. dbeaver)\n3. Inspect the rows returned to make sure they are what we expected. \n\nBut what do we expect? Sure, there's an expectation of the *shape* of the data we get back, i.e. the column names and maybe the number of rows are always the same, but the actual values within them can vary depending on what the current state of the database is - which ultimately can be affected by the time of day, period of the year, user behaviour i.e. the outside world (scary stuff). This isn't too bad for adhoc manual data checks but this doesn't work if we want to make testing simple and easy enough to integrate into an automated pipeline.\n\n# The End Goal\nHere's what we want to achieve (and will!):\n\n1. Spin up and connect to a clean db instance\n2. Insert some mock data necessary for our query to run\n3. Define the return data expected from running the query, given the mock data we have inserted \n4. Run our query that we want to test (i.e. `super_critical_sql_query` above)\n5. Assert that the results from `4.` match what we expected in `3.` \n6. Clean up the database and shut it down after our tests are complete.\n\nThe added advantage of `3.` is that we can now test edge cases without waiting for an opportunity to arise in the real world. So if there are rare data states in your database, you can test these out by inserting the \"rare\" data and running your query against it. We'll see how we can use pytest fixtures to make this setup effortless for multiple tests below! \n\n# Setting the scene\n\nThe full source code (app & tests) can be found [here](https://github.com/ismailmo1/pytest-sql)\n\nWe're currently working on a flask api with a single endpoint that returns the net calories for the day: \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n@app.get(\"/\")\ndef index():\n    start_date = request.args.get(\"start_date\")\n    end_date = request.args.get(\"end_date\")\n    if start_date and end_date:\n        calories = get_total_net_calories(\n            Session(engine),\n            tuple(\n                datetime.datetime.fromisoformat(d)\n                for d in [start_date, end_date]\n            ),\n        )\n    else:\n        calories = get_total_net_calories(Session(engine))\n    return jsonify({\"total_net_calories\": calories})\n```\n:::\n\n\n`get_total_calories` connects to a database that stores logs from a food diary and exercise data from my treadmill:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef get_total_net_calories(\n    session: Session,\n    date_range: tuple[date, date] = (\n        date.today(),\n        date.today() + timedelta(days=1),\n    ),\n) -> float | None:\n    start_date, end_date = date_range\n    query = \"\"\"\n        WITH consumption AS (\n            SELECT\n                sum(fr.\"kcal/unit\" * fc.qty) AS total_cons\n            FROM\n                public.food_cons fc\n                JOIN public.food_ref fr ON fc.food_id = fr.id\n            WHERE\n                date BETWEEN :start_date  :: timestamp\n                AND :end_date :: timestamp\n        ),\n        bike_exp AS (\n            SELECT\n            --we burn 1kcal/minute\n                sum(b.duration_min) AS total_bike_exp\n            FROM\n                bike b\n            WHERE\n                date BETWEEN :start_date :: timestamp\n                AND :end_date :: timestamp\n        )\n        SELECT\n            (consumption.total_cons - bike_exp.total_bike_exp) AS net_kcal\n        FROM\n            bike_exp,\n            consumption\n        \"\"\"\n    with session:\n        result: float | None = session.execute(  # type:ignore\n            query,\n            {\n                \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n                \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n            },\n        ).first()[0]\n    if result:\n        return round(result, 2)\n\n    return\n```\n:::\n\n\n# Testing Motivations\n\nIt's likely that the your application code will need to be changed at some point. For the net calorie API in our example, we can think of a scenario where we add a new exercise to our training regime (we got bored of just using the bike and have ventured out into using the treadmill). To ensure our API endpoint still returns accurate data, the expenditure from this new exercise should be considered in the net calorie calculation. A simple change could be to adjust our query so it looks something like this:\n\n```SQL\nWITH consumption AS (\n    SELECT\n        sum(fr.\"kcal/unit\" * fc.qty) AS total_cons\n    FROM\n        public.food_cons fc\n        JOIN public.food_ref fr ON fc.food_id = fr.id\n    WHERE\n        date BETWEEN :start_date  :: timestamp\n        AND :end_date :: timestamp\n),\nbike_exp AS (\n    SELECT\n    -- assume 1kcal/minute\n        sum(b.duration_min) AS total_bike_exp\n    FROM\n        bike b\n    WHERE\n        date BETWEEN :start_date :: timestamp\n        AND :end_date :: timestamp\n),\ntreadmill_exp AS (\n    SELECT\n    -- assume 0.5kcal/minute\n        sum(t.duration_min) * 0.5 AS total_treadmill_exp\n    FROM\n        treadmill t\n    WHERE\n        date BETWEEN :start_date :: timestamp\n        AND :end_date :: timestamp\n)\n\nSELECT\n    (consumption.total_cons - bike_exp.total_bike_exp - \n        treadmill_exp.total_bike_exp) AS net_kcal\nFROM\n    bike_exp,\n    treadmill_exp,\n    consumption\n```\n\nIf we follow the manual steps above, we could run this query directly on the database and do some manual calculations to make sure the net calories make sense. But there are some issues we could run into:\n\n1. We have limited treadmill data (or none) to test our query since this is a new data source.\n2. We can't easily test how our query will handle edge cases since they're rare or don't exist yet.\nSome edge cases we would want to test: \n    a. no treadmill data between the start and end dates\n    b. no bike data between the start and end dates\n    c. all true/false combinations of a. and b.\n    d. expenditure is higher than consumption\n3. Syntax errors/typos could be introduced in the process of copy/pasting between the SQL client and the application.\n\n# Automated testing\n\nHopefully there's enough motivation to look for an automated and repeatable way to test the query, so we can start implementing the [end goal](#the-end-goal) mentioned above. \n\n## Instantiate the test database\n\nBefore doing any querying or testing, we need to start a database instance - docker makes this quite painless. \n\nWe'll define the image in a Dockerfile and add some seed data that we'll need for our query. We could've chosen to add this data in our tests or fixtures, but since this is reference data and is not specific to a test, we can add it in during the image build. \n\nThe seed data script creates the table and adds the reference data: \n\n```SQL\nCREATE TABLE public.food_ref (\n    id varchar NOT NULL,\n    \"name\" varchar NULL,\n    \"kcal/unit\" int4 NULL,\n    unit_of_measure varchar NULL,\n    CONSTRAINT food_ref_pkey PRIMARY KEY (id)\n);\n\nINSERT INTO\n    public.food_ref (id, \"name\", \"kcal/unit\", unit_of_measure)\nVALUES\n('a', 'bread', 10, 'g')\n-- we can add more values here;\n```\nThe postgres docker image will run any .sql (or .sh) scripts in docker-entrypoint-initdb.d/ so we can copy our seed script there:\n```Dockerfile\nFROM postgres:10-alpine\nENV POSTGRES_USER=postgres\nENV POSTGRES_PASSWORD=password\nEXPOSE 5432\n# seed database with food reference data\nCOPY ./seed_scripts/food_ref.sql /docker-entrypoint-initdb.d/\n```\n\n## Setup fixtures\nWe can define a session-scope fixture which uses the docker library to build the image and run the container for our tests. Using yield here means that once the tests have run, control is returned back to this fixture and we can remove the container and image so we return to a clean state.\n```python\n@pytest.fixture(scope=\"session\")\ndef postgres_container():\n    \"\"\"start docker instance of postgres\"\"\"\n    client = docker.from_env()\n\n    client.images.build(\n        path=TEST_DB_DOCKERFILE_PATH, tag=\"test_postgres\", nocache=True\n    )\n\n    try:\n        # create container\n        db_container = client.containers.run(\n            \"test_postgres\",\n            name=\"test_db\",\n            detach=True,\n            ports={5432: 5432},\n        )\n\n    except APIError:\n        # docker api returns an error sincewe already have a container\n        # remove existing container and make a new one\n        db_container = client.containers.get(\"test_db\")\n        db_container.remove()  # type:ignore\n        db_container = client.containers.run(\n            \"test_postgres\",\n            name=\"test_db\",\n            detach=True,\n            ports={5432: 5432},\n        )\n    # return to calling function so we can use the container\n    yield\n    # calling function is done so we can stop and remove the container\n    db_container.stop()  # type:ignore\n    db_container.remove()  # type:ignore\n```\n\nThe only other setup we need is to create a fixture for a session that runs on the test database container above, this will allow us to define some steps to connect, run our tests and then cleanup the database to remove all the data we added for our test:\n\n```python\n@pytest.fixture\ndef test_session(postgres_container) -> Iterator[Session]:\n    \"\"\"create db session and cleanup data after each test\"\"\"\n    # testing connection string\n    DOCKER_USERNAME = \"postgres\"\n    DOCKER_PASSWORD = \"password\"\n    DOCKER_HOSTNAME = \"localhost:5432\"\n    DOCKER_DB_NAME = \"postgres\"\n\n    sql_url = (\n        f\"postgresql://{DOCKER_USERNAME}:{DOCKER_PASSWORD}@\"\n        f\"{DOCKER_HOSTNAME}/{DOCKER_DB_NAME}\"\n    )\n    engine = create_engine(sql_url, pool_pre_ping=True)\n\n    # wait until db is ready\n    MAX_RETRIES = 100\n    #  max value for backoff time\n    MAX_RETRY_SLEEP_SEC = 2\n\n    num_retries = 0\n    try:\n        while num_retries < MAX_RETRIES:\n            num_retries += 1\n            # keep increasing back off time until MAX_RETRY_SLEEP_SEC\n            # using (truncated) exponential backoff\n            sleep_time_ms = min(\n                [MAX_RETRY_SLEEP_SEC * 1000, 100 * num_retries]\n            )\n            try:\n                # check if db is ready with simple query\n                engine.execute(\"SELECT 1\")\n            except OperationalError:\n                # db is still starting up, continue loop and retry\n                sleep(sleep_time_ms / 1000)\n                continue\n            # db has started - lets break out of loop\n            break\n    except OperationalError:\n        raise Exception(\"Couldn't connect to Test Postgres Docker Instance!\")\n\n    # create all tables registered to our declarative base class\n    Base.metadata.create_all(engine)\n    with Session(engine) as session:\n        yield session\n        session.commit()\n\n    # clean up our db\n    Base.metadata.drop_all(engine)\n\n```\n\nThe logic in `ln16` - `ln38` ensures that we do not try to create our session before the database in the container is ready. The `client.containers.run` function in the `postgres_container` fixture returns before the database is ready since the container has already started successfully.\n\n## The payoff\n\nFinally we can actually test our query: \n\n```python\ndef test_get_net_calories(test_session: Session):\n    food_cons = FoodConsumption(\n        food_id=\"a\", qty=\"100\", date=datetime.date.today()\n    )\n    bike_session = Bike(\n        date=datetime.date.today(), speed_mph=5, duration_min=20\n    )\n    with test_session:\n        test_session.add_all([food_cons, bike_session])\n        test_session.commit()\n\n    result = get_total_net_calories(test_session)\n\n    assert result == 980\n```\nBreaking down what happens when we run our test:\n\n1. `ln1` we pass in `test_session` to our test which call the `test_session` fixture above\n2. The `test_session` fixture call the `postgres_container` fixture\n3. The db image is built, seeded with reference data and the container is started\n4. A session is created after connecting to the db container\n5. `ln2` - `ln10` we add some data specific to our test (ARRANGE)\n6. `ln12` we run our function under test and pass it the `test_session` (ACT)\n7. `ln14` we ensure the data is what we expect (ASSERT)  \n8. Return to the `test_session` fixture and delete all data in the db (i.e. undo step 5.)\n9. Return to the `postgres_container` fixture and stop the container and remove it.\n10. ???\n11. PROFIT!!!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}