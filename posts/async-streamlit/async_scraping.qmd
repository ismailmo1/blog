---
title: Async web scraping and Streamlit
description: Speeding up web scraping in a Streamlit analytics app
date: 21-12-2022
categories:
  - python
  - streamlit
  - asyncio
  - web-scraping
execute:
  eval: false
image: pytest_sql.png
format:
    html:
        toc: true
        toc-location: left
        number-sections: true
        code-line-numbers: true
jupyter: python3
---

Earlier this year, I built [myfitnesspal wrapped](https://wrapped.ismailmo.com): inspired by Spotify's famous wrapped campaign, this web app scrapes all your data from myfitnesspal (a food tracking app), processes and analyses the data, and finally gives some cool (in my opinion) statistics and charts about your dietary habits.

![myfitnesspal wrapped analysis](https://github.com/ismailmo1/mfp-wrapped/raw/develop/app/images/card_preview.png)

While myfitnesspal does have an API - it requires you to fill out and application form and unfortunately I was unsuccessful in my application. Luckily they do not rate limit requests to the website, so although it wouldn't be as friendly as a well-formatted JSON response, there was still a way to get all the data.

On observation of the network requests in the browser, the food diary page is rendered server side, so our last hope of mimicing any API calls was dead - but atleast the url was easy to reason about: just a request to the `diary/{user}` endpoint with a query param of the date of the food diary.

![snooping around the myfitnesspal network tab](./mfp-network-tab.png)

In the spirit of actually finishing projects I started with a simple (naive) approach of looping through all the dates within the date range and making a request to the corresponding date:
```{python}
import pandas as pd
import asyncio
import time
```

Define a 2 month date range to give us enough requests to benchmark on 

```{python}
dates = pd.date_range("2022-10-01", "2022-11-29")
```

Try the standard way of one request at a time

```{python}
import requests

start_time = time.perf_counter()

sesh = requests.Session()

def scrape_diary(user, date, client):
    url = f"https://www.myfitnesspal.com/food/diary/{user}?date={date}"
    res = client.get(url)
    return res.text
diaries =[]
for date in dates:
    diaries.append(scrape_diary("ismailmo", date, sesh))

kcals = []
for diary in diaries:
    kcals.append(pd.read_html(diary, flavor="lxml")[0].iloc[-4,1])

elapsed = time.perf_counter() - start_time

print(f"time to scrape data: {elapsed:.2f} seconds")
```

```{python}
from httpx import AsyncClient

start_time = time.perf_counter()

async_client = AsyncClient()
date_param = "2022-11-29"
user = "ismailmo"

async def async_scrape_diary(user, date, client):
    url = f"https://www.myfitnesspal.com/food/diary/{user}?date={date}"
    res = await client.get(url)
    return date, res.text
coros = []
async_diaries =[]
for date in dates:
    coros.append(async_scrape_diary("ismailmo", date, async_client))

async_diaries = await asyncio.gather(*coros)

async_kcals = []
for date, diary in async_diaries:
    async_kcals.append(pd.read_html(diary, flavor="lxml")[0].iloc[-4,1])

async_elapsed = time.perf_counter() - start_time

print(f"time to scrape data: {async_elapsed:.2f} seconds")
```

```{python}
print(f"Speed up of {((elapsed - async_elapsed)/ elapsed) * 100:.2f}%")
```

This is a huge increase in speed and it's more significant as the size of the date range is greater (more pages scraped and more requests made), lets make sure the data we get back is the same:

```{python}
async_kcals == kcals
```

